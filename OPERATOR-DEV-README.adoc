= Coherence Operator

This module is the Coherence Operator implementation. It is a Go based project built using the
https://github.com/operator-framework/operator-sdk[Operator SDK].

== Prerequisites

* https://github.com/operator-framework/operator-sdk/tree/v0.9.0[operator-sdk] version *v0.9.0*
* https://git-scm.com/downloads[git]
* https://golang.org/dl/[go] version v1.12+.
* https://www.mercurial-scm.org/downloads[mercurial] version 3.9+
* https://docs.docker.com/install/[docker] version 17.03+.
* https://kubernetes.io/docs/tasks/tools/install-kubectl/[kubectl] version v1.11.3+.
* Access to a Kubernetes v1.11.3+ cluster.

* Optional: https://github.com/go-delve/delve/tree/master/Documentation/installation[delve]
version 1.2.0+ (for local debugging with `operator-sdk up local --enable-delve`).


* This project uses `make` for building, which should already be installed on most systems

_Note:_ This project currently uses the Operator SDK v0.9.0 so make sure you install the correct version of the CLI.

_Note:_ As stated above this project requires K8s v1.11.3+ which is not the version currently installed by
Docker on MacOS. To run the Coherence Operator on a Mac you need to have the Edge version of Docker for MacOS
that currently uses k8s v1.14.1.


== Building:

The Operator SDK generates Go projects that use Go Modules and hence the Coherence Operator uses Go Modules too.
The Coherence Operator can be checked out from Git to any location, it does not have to be under your `$GOPATH`.
The first time that the project is built may require Go to fetch a number of dependencies and may take longer than
usual to complete.

The easiest way to build the whole project is using `make`.
To build the Coherence Operator, package the Helm charts and create a Docker image
run the following from the `operator` directory:

[source,bash]
----
make build
----

It is also possible to run `make` from the root directory of the Coherence Operator project using the command:

[source,bash]
----
make -C operator build
----

The `-C operator` parameters cause `make` to run in the `operator` directory. These parameters can be applied
to any of the make commands in this document if running them from the root directory of the Coherence Operator
project.

=== Testing

The Coherence Operator contains tests that can be executed using `make`. The tests are plain Go tests and
also https://github.com/onsi/ginkgo[Ginkgo] test suites.

To execute the unit and functional tests that do not require a k8s cluster you can execute the following command:
[source,bash]
----
make test
----
This will build and execute all of the tests, you do not need to have run a `make build` first.


==== Build Versions

By default the version number used to tag the Docker image and Helm charts is set in the `VERSION` property
in the `Makefile`. To build a different version set the `VERSION` environment variable before building.
Setting the version will cause the version value to be used for the Docker image tag as well as in the k8s
yaml files and Helm charts.

For example to build version `2.0.0`
[source,bash]
----
export VERSION=2.0.0
make build
----

== Running the Coherence Operator

There are two ways to run the Coherence Operator, either deployed into a k8s cluster or by using the Operator SDK
to run it locally on your dev machine (assuming your dev machine has access to a k8s cluster such as Docker Desktop
on MacOS).

=== Namespaces
*NOTE:* The Coherence Operator by default runs in and monitors a *single* namespace.
This is different behaviour to v1.0 of the Coherence Operator.
For more details see the Operator SDK document on
https://github.com/operator-framework/operator-sdk/blob/v0.9.0/doc/operator-scope.md[Operator Scope].


=== Install the CRDs

Prior to any testing the CRDs need to be installed in the k8s cluster. Although the Operator runs in a single
namespace CRDs are a global (non-namespaced) resource. The simplest way to install the CRDs is to run the
provided shell script:
[source,bash]
----
./hack/install.sh
----
This script will first delete any old installs of the CRDs and then install the new versions.


=== Running Locally

During development running the Coherence Operator locally is by far the simplest option as it is faster and
it also allows remote debugging if you are using a suitable IDE.

The https://github.com/operator-framework/operator-sdk/blob/v0.9.0/doc/sdk-cli-reference.md[Operator SDK CLI]
has commands to run the operator locally using `operator sdk up local` for example:
[source,bash]
----
export OPERATOR_NAME=coherence-operator
operator-sdk up local --namespace=default \
    --operator-flags="--watches-file=local-watches.yaml"
----
The Operator SDK requires the `OPERATOR_NAME` environment variable to be set.
The command above will run the operator using the `default` namespace. To change to another namespace
just change the `--namespace` option
(See the https://github.com/operator-framework/operator-sdk/blob/v0.9.0/doc/sdk-cli-reference.md#up[Operator SDK CLI]
documentation for all of the parameters to the `up` command).

==== Stopping the Local Operator
To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in
this way. To make sure all of the processes are dead you can run the kill script:
[source,bash]
----
./hack/kill-local.sh
----

=== Clean-up

After running the operato the CRDs can be removed from the k8s cluster by running the clean-up script:
[source,bash]
----
./hack/cleanup.sh
----

=== Debugging Locally

When running locally in development it is often useful to be able to attach a debugger to the running code.
To do this you need to have https://github.com/go-delve/delve/tree/master/Documentation/installation[delve]
installed and then add the `--enable-delve` parameter to the `operator-sdk up` command.
[source,bash]
----
export OPERATOR_NAME=coherence-operator
operator-sdk up local --namespace=default \
    --operator-flags="--watches-file=local-watches.yaml" \
    --enable-delve
----
This will start the operator in debug mode; the Go code will pause until a debugger connects on port 2345 which
is the default debug port.

A simpler way to run the operator in debug mode is to use the shell script:
[source,bash]
----
./hack/debug.sh
----
As well as running the same `operator-sdk up` commands as above it also pipes the output to both the console
and to the file `operator.out`


== Project Structure

This project was initially generated using the Operator SDK and this dictates the structure of the project
which means that files and directories should not be moved arbitrarily.

=== Operator SDK Files
The following should not be moved:

|===
|File |Description

|`bin/` |scripts used in the Operator Docker image 
|`build/Dockerfile` |the `Dockerfile` used by the Operator SDK to build the Docker image 
|`cmd/manager/main.go` |The Operator `main` generated by the Operator SDK 
|`deploy/` |Yaml files generated and maintained by the Operator SDK 
|`deploy/crds` |The CRD files generated and maintained by the Operator SDK 
|`helm-charts/` |The Helm charts used by the Operator 
|`pkg/apis` |The API `struct` code generated by the Operator SDK and used to generate the CRD files 
|`pkg/controller` |The controller code generated by the Operator SDK
|`watches.yaml` |The Helm Operator configuration generated by the Operator SDK
|`local-watches.yaml` |The Helm Operator configuration used when running the operator locally
|===


== Useful Info

=== Labeling Your K8s Node

For local testing, for example in Docker Desktop it is useful to add the zone label to your local K8s node with
the fault domain that is then used by the Coherence Pods to set their `zone` property.

For example, if your local node is called `docker-desktop` you can use the following command to set
the zone name to `twilight-zone`:
[source,bash]
----
kubectl label node docker-desktop failure-domain.beta.kubernetes.io/zone=twilight-zone
----
With this label set all Coherence Pods installed by the Coherence Operator on that node will be
running in the `twilight-zone`.


=== Kubernetes Dashboard

Assuming that you have the https://github.com/kubernetes/dashboard[Kubernetes Dashboard] then you can easily
start the local proxy and display the required login token by running:
[source,bash]
----
./hack/kube-dash.sh
----
This will display the authentication token, the local k8s dashboard URL and then start `kubectl proxy`.

