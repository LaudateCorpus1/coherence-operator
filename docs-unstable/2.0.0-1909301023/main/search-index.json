{
    "docs": [
        {
            "location": "/developer/06_debugging",
            "text": " To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >make debug-stop ",
            "title": "Stopping the Debug Session"
        },
        {
            "location": "/developer/06_debugging",
            "text": " To debug the operator while running a particular tests first start the debugger as described above. Then use the debug make test target to execute the test. For example to debug the TestMinimalCoherenceCluster test first start the debug session: <markup lang=\"bash\" >make run-debug Then execute the test with the debug-e2e-local-test make target: <markup lang=\"bash\" >make debug-e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' ",
            "title": "Debugging Tests"
        },
        {
            "location": "/developer/06_debugging",
            "text": " Assuming that you have an IDE capable of debugging Go and have delve installed you can debug the operator. When debugging an instance of the operator is run locally so functionality that will only work when the operator is deployed into k8s cannot be properly debugged. To start an instance of the operator that can be debugged use the make target run-debug , for example: <markup lang=\"bash\" >make run-debug This will start the operator and listen for a debugger to connect on the default delve port 2345 . The operator will connect to whichever k8s cluster the current environment is configured to point to. Stopping the Debug Session To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >make debug-stop Debugging Tests To debug the operator while running a particular tests first start the debugger as described above. Then use the debug make test target to execute the test. For example to debug the TestMinimalCoherenceCluster test first start the debug session: <markup lang=\"bash\" >make run-debug Then execute the test with the debug-e2e-local-test make target: <markup lang=\"bash\" >make debug-e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' ",
            "title": "Debugging the Coherence Operator"
        },
        {
            "location": "/install/03_helm_install",
            "text": " The simplest way to install the Coherence Operator is to use the Helm chart. This will ensure that all of the correct resources are created in Kubernetes. ",
            "title": "preambule"
        },
        {
            "location": "/install/03_helm_install",
            "text": " Add the coherence helm repository using the following commands: <markup lang=\"bash\" >helm repo add coherence https://oracle.github.io/coherence-operator/charts helm repo update ",
            "title": "Add the Coherence Helm Repository"
        },
        {
            "location": "/install/03_helm_install",
            "text": " To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Uninstall the Coherence Operator Helm chart"
        },
        {
            "location": "/install/03_helm_install",
            "text": " Once the Coherence Helm repo is configured the Coherence Operator can be installed using a normal Helm install command: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ coherence/coherence-operator where &lt;namespace&gt; is the namespace that the Coherence Operator will be installed into and the namespace where it will manage CoherenceClusters Uninstall the Coherence Operator Helm chart To uninstall the operator: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Install the Coherence Operator Helm chart"
        },
        {
            "location": "/clusters/070_private_repos",
            "text": " Sometimes the images used by a Coherence cluster need to be pulled from a private image registry that requires credentials. The Coherence Operator supports supplying credentials in the CoherenceCluster configuration. The Kubernetes documentation on using a private registries gives a number of options for supplying credentials. ",
            "title": "Using Private Image Registries"
        },
        {
            "location": "/clusters/070_private_repos",
            "text": " Kubernetes supports configuring pods to use imagePullSecrets for pulling images. If possible, this is the preferable and most portable route. See the kubernetes docs for this. Once secrets have been created in the namespace that the CoherenceCluster is to be installed in then the secret name can be specified in the CoherenceCluster spec . It is possible to specify multiple secrets in the case where the different images being used are pulled from different registries. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: imagePullSecrets: - coherence-secret The coherence-secret will be used for pulling images from the registry associated to the secret The imagePullSecrets field is a list of string values so multiple secrets can be specified for different authenticated registries in the case where the Coherence cluster will use images from different authenticated registries.. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: imagePullSecrets: - coherence-secret - ocr-secret The imagePullSecrets list specifies two secrets to use coherence-secret and ocr-secret Image pull secrets are only specified for the CoherenceCluster as a whole as there is no benefit to being able to specify different secrets for different roles within a cluster. ",
            "title": "Use ImagePullSecrets"
        },
        {
            "location": "/developer/09_useful",
            "text": " For local testing, for example in Docker Desktop it is useful to add the zone label to your local K8s node with the fault domain that is then used by the Coherence Pods to set their site property. For example, if your local node is called docker-desktop you can use the following command to set the zone name to twilight-zone : <markup lang=\"bash\" >kubectl label node docker-desktop failure-domain.beta.kubernetes.io/zone=twilight-zone With this label set all Coherence Pods installed by the Coherence Operator on that node will be running in the twilight-zone . ",
            "title": "Labeling Your K8s Node"
        },
        {
            "location": "/developer/09_useful",
            "text": " Assuming that you have the Kubernetes Dashboard then you can easily start the local proxy and display the required login token by running: <markup lang=\"bash\" >./hack/kube-dash.sh This will display the authentication token, the local k8s dashboard URL and then start kubectl proxy . ",
            "title": "Kubernetes Dashboard"
        },
        {
            "location": "/developer/09_useful",
            "text": " Sometimes a CoherenceInternal resource becomes stuck in k8s. This is because the operator adds finalizers to the resources causing k8s to be unable to delete them. The simplest way to delete them is to use the kubectl patch command to remove the finalizer. For example, if there was a CoherenceInternal resource called test-role in namespace testing then the following command could be used. <markup lang=\"bash\" >kubectl -n testing patch coherenceinternal/test-role \\ -p '{\"metadata\":{\"finalizers\": []}}' \\ --type=merge; Alternatively there is a make target that wil clean up and remove all CoherenceCLuster, CoherenceRole and CoherenceInternal resources from the test namespace. <markup lang=\"bash\" >make delete-coherence-clusters ",
            "title": "Stuck CoherenceInternal Resources"
        },
        {
            "location": "/developer/09_useful",
            "text": " Labeling Your K8s Node For local testing, for example in Docker Desktop it is useful to add the zone label to your local K8s node with the fault domain that is then used by the Coherence Pods to set their site property. For example, if your local node is called docker-desktop you can use the following command to set the zone name to twilight-zone : <markup lang=\"bash\" >kubectl label node docker-desktop failure-domain.beta.kubernetes.io/zone=twilight-zone With this label set all Coherence Pods installed by the Coherence Operator on that node will be running in the twilight-zone . Kubernetes Dashboard Assuming that you have the Kubernetes Dashboard then you can easily start the local proxy and display the required login token by running: <markup lang=\"bash\" >./hack/kube-dash.sh This will display the authentication token, the local k8s dashboard URL and then start kubectl proxy . Stuck CoherenceInternal Resources Sometimes a CoherenceInternal resource becomes stuck in k8s. This is because the operator adds finalizers to the resources causing k8s to be unable to delete them. The simplest way to delete them is to use the kubectl patch command to remove the finalizer. For example, if there was a CoherenceInternal resource called test-role in namespace testing then the following command could be used. <markup lang=\"bash\" >kubectl -n testing patch coherenceinternal/test-role \\ -p '{\"metadata\":{\"finalizers\": []}}' \\ --type=merge; Alternatively there is a make target that wil clean up and remove all CoherenceCLuster, CoherenceRole and CoherenceInternal resources from the test namespace. <markup lang=\"bash\" >make delete-coherence-clusters ",
            "title": "Useful Info"
        },
        {
            "location": "/guides/100_logging",
            "text": " Coherence clusters logs can be captured with ELK (or more accurately EFK) using Elasticsearch, Fluentd and Kibana. ",
            "title": "preambule"
        },
        {
            "location": "/guides/100_logging",
            "text": " TBD&#8230;&#8203; ",
            "title": "Capturing Coherence Cluster Logs"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The CoherenceCluster CRD is the main CRD that defines what a Coherence cluster looks like. This is the CRD that a customer creates and manges through the normal kubernetes commands and APIs. A CoherenceCluster is made up of one or more roles. Each role defines a sub-set of the members of a Coherence cluster (or all of the members in the case of a cluster with a single role). The yaml for the CoherenceCluster CRD is in the file deploy/crds/coherence_v1_coherencecluster_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceCluster struct in the pkg/apis/coherence/v1/coherencecluster_types.go source file. ",
            "title": "CoherenceCluster CRD"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The CoherenceRole CRD is a definition of a role within a CoherenceCluster. A role is a sub-set of the members of a cluster that all share the same configuration. A customer should not interact directly with a CoherenceRole other than when scaling (for example using kubectl scale commands). The reason that a cluster is split into roles represented by a different CRD is to allow more fine grained control over different parts of the cluster, especially for operations such as scaling. By having a separate CRD for a role allows a customer to update or scale each role individually. The yaml for the CoherenceRole CRD is in the file deploy/crds/coherence_v1_coherencerole_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceRole struct in the pkg/apis/coherence/v1/coherencerole_types.go source file. ",
            "title": "CoherenceRole CRD"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The CoherenceInternal CRD is (as the name suggests) entirely internal to the Coherence Operator and a customer should not interact with it at all. The CoherenceInternal CRD is a representation of the values file used to install the Coherence Helm chart. The yaml for the CoherenceInternal CRD is in the file deploy/crds/coherence_v1_coherenceinternal_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceInternal struct in the pkg/apis/coherence/v1/coherenceinternal_types.go source file. ",
            "title": "CoherenceInternal CRD"
        },
        {
            "location": "/developer/03_high_level",
            "text": " In Kubernetes a CRD is a yaml (or json) file that defines the structure of a custom resource. When building operators using the Operator SDK the yaml files are not edited directly, they are generated from the Go structs in the source code. The Coherence Operator has three CRDs: CoherenceCluster CoherenceRole CoherenceInternal CoherenceCluster CRD The CoherenceCluster CRD is the main CRD that defines what a Coherence cluster looks like. This is the CRD that a customer creates and manges through the normal kubernetes commands and APIs. A CoherenceCluster is made up of one or more roles. Each role defines a sub-set of the members of a Coherence cluster (or all of the members in the case of a cluster with a single role). The yaml for the CoherenceCluster CRD is in the file deploy/crds/coherence_v1_coherencecluster_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceCluster struct in the pkg/apis/coherence/v1/coherencecluster_types.go source file. CoherenceRole CRD The CoherenceRole CRD is a definition of a role within a CoherenceCluster. A role is a sub-set of the members of a cluster that all share the same configuration. A customer should not interact directly with a CoherenceRole other than when scaling (for example using kubectl scale commands). The reason that a cluster is split into roles represented by a different CRD is to allow more fine grained control over different parts of the cluster, especially for operations such as scaling. By having a separate CRD for a role allows a customer to update or scale each role individually. The yaml for the CoherenceRole CRD is in the file deploy/crds/coherence_v1_coherencerole_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceRole struct in the pkg/apis/coherence/v1/coherencerole_types.go source file. CoherenceInternal CRD The CoherenceInternal CRD is (as the name suggests) entirely internal to the Coherence Operator and a customer should not interact with it at all. The CoherenceInternal CRD is a representation of the values file used to install the Coherence Helm chart. The yaml for the CoherenceInternal CRD is in the file deploy/crds/coherence_v1_coherenceinternal_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceInternal struct in the pkg/apis/coherence/v1/coherenceinternal_types.go source file. ",
            "title": "Custom Resource Definitions (CRDs)"
        },
        {
            "location": "/developer/03_high_level",
            "text": " To modify the contents of a CRD (for example to add a new field) the corresponding Go struct needs to be updated. For backwards compatibility between released versions we should ensure that we do not delete fields. After any of the structs have been modified the new CRD files need to be generated, this is done by running the Operator SDK generator using the Makefile. If the generate step is not run the code will not work properly. <markup lang=\"bash\" >make generate ",
            "title": "Modifying CRDs"
        },
        {
            "location": "/developer/03_high_level",
            "text": " The Coherence Operator has been built using the Operator SDK and hence the design is based on how the framework works. Custom Resource Definitions (CRDs) In Kubernetes a CRD is a yaml (or json) file that defines the structure of a custom resource. When building operators using the Operator SDK the yaml files are not edited directly, they are generated from the Go structs in the source code. The Coherence Operator has three CRDs: CoherenceCluster CoherenceRole CoherenceInternal CoherenceCluster CRD The CoherenceCluster CRD is the main CRD that defines what a Coherence cluster looks like. This is the CRD that a customer creates and manges through the normal kubernetes commands and APIs. A CoherenceCluster is made up of one or more roles. Each role defines a sub-set of the members of a Coherence cluster (or all of the members in the case of a cluster with a single role). The yaml for the CoherenceCluster CRD is in the file deploy/crds/coherence_v1_coherencecluster_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceCluster struct in the pkg/apis/coherence/v1/coherencecluster_types.go source file. CoherenceRole CRD The CoherenceRole CRD is a definition of a role within a CoherenceCluster. A role is a sub-set of the members of a cluster that all share the same configuration. A customer should not interact directly with a CoherenceRole other than when scaling (for example using kubectl scale commands). The reason that a cluster is split into roles represented by a different CRD is to allow more fine grained control over different parts of the cluster, especially for operations such as scaling. By having a separate CRD for a role allows a customer to update or scale each role individually. The yaml for the CoherenceRole CRD is in the file deploy/crds/coherence_v1_coherencerole_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceRole struct in the pkg/apis/coherence/v1/coherencerole_types.go source file. CoherenceInternal CRD The CoherenceInternal CRD is (as the name suggests) entirely internal to the Coherence Operator and a customer should not interact with it at all. The CoherenceInternal CRD is a representation of the values file used to install the Coherence Helm chart. The yaml for the CoherenceInternal CRD is in the file deploy/crds/coherence_v1_coherenceinternal_crd.yaml . This yaml is generated by the Operator SDK from the CoherenceInternal struct in the pkg/apis/coherence/v1/coherenceinternal_types.go source file. Modifying CRDs To modify the contents of a CRD (for example to add a new field) the corresponding Go struct needs to be updated. For backwards compatibility between released versions we should ensure that we do not delete fields. After any of the structs have been modified the new CRD files need to be generated, this is done by running the Operator SDK generator using the Makefile. If the generate step is not run the code will not work properly. <markup lang=\"bash\" >make generate ",
            "title": "High Level Design"
        },
        {
            "location": "/clusters/010_introduction",
            "text": " Creating a Coherence cluster using the Coherence Operator is as simple as creating any other Kubernetes resource. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/010_introduction",
            "text": " The Coherence Operator uses a Kubernetes CustomResourceDefinition named CoherenceCluster to define the spec for a Coherence cluster. All of the fields of the CoherenceCluster crd are optional so a Coherence cluster can be created with yaml as simple as the following: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster The metadata.name field will be used as the Coherence cluster name. The yaml above will create a Coherence cluster with three storage enabled members. There is not much that can actually be achived with this cluster because no ports are exposed outside of Kubernetes so the cluster is inaccessible. It could be accessed by other Pods in the same ",
            "title": "Create CoherenceCluster Resources"
        },
        {
            "location": "/clusters/010_introduction",
            "text": " A role is what is actually configured in the CoherenceCluster spec. In a traditional Coherence application that may have had a number of storage enabled members and a number of storage disable Coherence*Extend proxy members this cluster would have effectively had two roles, \"storage\" and \"proxy\". Some clusters may simply have just a storage role and some complex Coherence applications and clusters may have many roles and even different roles storage enabled for different caches/services within the same cluster. ",
            "title": "Coherence Roles"
        },
        {
            "location": "/clusters/020_k8s_resources",
            "text": " When a CoherenceCluster is deployed into Kubernetes the Coherence Operator will create a number of other resources in Kubernetes. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/020_k8s_resources",
            "text": " A CoherenceCluster is made up of one or more roles. In theory a CoherenceCluster could have zero roles but this would not by typical. A role maps to zero or more Pods that will all share the same specification and hence typically take on the same business role within an application. In Kubernetes a Coherence role is represented by a CoherenceRole and a CoherenceInternal crd although it is not expected that these crds are modified directly, they are purely used to allow roles in the same cluster to be managed as independent entities by the Coherence Operator. When a resource of type CoherenceCluster is created in Kubernetes the Coherence Operator will create the other resources. A Service will be created for every CoherenceCluster to be used for Coherence WKA (cluster membership discovery). Every Pod that is created as part of this cluster will have a label coherenceCluster=&lt;cluster-name&gt; and the WKA Service uses this label to identify all of the Pods in the same Coherence cluster. The Pods then use the Service as their WKA address. A CoherenceRole resource will be created for each role in the CoherenceCluster spec that has a replica count greater than zero. The name of the CoherenceRole will be in the form &lt;cluster-name&gt;-&lt;role-name&gt; Each CoherenceRole will have a related CoherenceInternal resource. The name of the CoherenceInternal will be the same as the CoherenceRole resource. Each CoherenceRole will have a related StatefulSet with corresponding Pods and headless Service required by the StatefulSet . The name of the StatefulSet will be the same as the CoherenceRole For each port that a role in a CoherenceCluster is configured to expose a corresponding Service will be created for that port. The name of the Service will be &lt;cluster-name&gt;-&lt;role-name&gt;-&lt;port-name&gt; (although this can be overridden when specifying the port in the CoherenceCLuster spec for that role and port). ",
            "title": "Kubernetes Resource Relationships When Creating Coherence Clusters"
        },
        {
            "location": "/clusters/100_logging",
            "text": " There are various settings in a Coherence role that control different aspects of logging, including the Coherence log level, configuration files and whether Fluentd log capture is enabled. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The Coherence log level is set with the logging.level field. This field is an integer value between zero and nine (see the Coherence documentation for a fuller explanation). To set the Coherence log level when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: level: 5 The implicit role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data logging: level: 9 - role: proxy logging: level: 5 The data role will have a Coherence log level of 9 The proxy role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: level: 9 roles: - role: data - role: proxy - role: web logging: level: 5 The data and proxy roles will use the default Coherence log level of 9 The web role overrides the default Coherence log level setting it to 5 ",
            "title": "Coherence Log Level"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The default logging configuration for Coherence clusters started by the Coherence Operator is to set Coherence to used JDK logging; the JDK logger is then configured with a configuration file. The default configuration file is embedded into the Pod by the Coherence Operator but this default my be overridden; for example an application deployed into the cluster may require different logging configurations. The name of the file is provided in the logging.configFile field. The logging configuration file must be available to the JVM when it starts, either by providing it in application code or by mounting a volume containing the file, or by using a ConfigMap . To set the logging configuration file when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties The implicit role will use the app-logging.properties logging configuration file To set the logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data logging: configFile: data-logging.properties - role: proxy logging: configFile: proxy-logging.properties The data role will use the data-logging.properties logging configuration file The proxy role will use the proxy-logging.properties logging configuration file To set a default logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties roles: - role: data - role: proxy - role: web logging: configFile: web-logging.properties The app-logging.properties logging configuration file is set as the default ans will be used by the data and proxy roles. The web role has a specific configuration file set and will use the web-logging.properties file ",
            "title": "Logging Config File"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The logging.ConfigMap field can be used to specify the name of a ConfigMap that contains the logging configuration file to use. The ConfigMap should exist in the same namespace as the Coherence cluster. TBD&#8230;&#8203; ",
            "title": "Logging ConfigMap"
        },
        {
            "location": "/clusters/100_logging",
            "text": " Logging configuration for a role is defined in the logging section of the role&#8217;s spec . There are a number of different fields used to configure different logging features. The logging configuration can be set at different places depending on whether the implicit role or explicit roles are being configured. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: level: 9 configFile: app-logging.properties configMapName: logging-cm fluentd: enabled: true image: fluent/fluentd-kubernetes-daemonset:v1.3.3-debian-elasticsearch-1.3 imagePullPolicy: IfNotPresent configFile: fluentd-config.yaml tag: test-cluster The fields in the example above are described in detail in the following sections. Coherence Log Level The Coherence log level is set with the logging.level field. This field is an integer value between zero and nine (see the Coherence documentation for a fuller explanation). To set the Coherence log level when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: level: 5 The implicit role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data logging: level: 9 - role: proxy logging: level: 5 The data role will have a Coherence log level of 9 The proxy role will have a Coherence log level of 5 To set the log level for explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: level: 9 roles: - role: data - role: proxy - role: web logging: level: 5 The data and proxy roles will use the default Coherence log level of 9 The web role overrides the default Coherence log level setting it to 5 Logging Config File The default logging configuration for Coherence clusters started by the Coherence Operator is to set Coherence to used JDK logging; the JDK logger is then configured with a configuration file. The default configuration file is embedded into the Pod by the Coherence Operator but this default my be overridden; for example an application deployed into the cluster may require different logging configurations. The name of the file is provided in the logging.configFile field. The logging configuration file must be available to the JVM when it starts, either by providing it in application code or by mounting a volume containing the file, or by using a ConfigMap . To set the logging configuration file when defining the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties The implicit role will use the app-logging.properties logging configuration file To set the logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data logging: configFile: data-logging.properties - role: proxy logging: configFile: proxy-logging.properties The data role will use the data-logging.properties logging configuration file The proxy role will use the proxy-logging.properties logging configuration file To set a default logging configuration file when defining explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: logging: configFile: app-logging.properties roles: - role: data - role: proxy - role: web logging: configFile: web-logging.properties The app-logging.properties logging configuration file is set as the default ans will be used by the data and proxy roles. The web role has a specific configuration file set and will use the web-logging.properties file Logging ConfigMap The logging.ConfigMap field can be used to specify the name of a ConfigMap that contains the logging configuration file to use. The ConfigMap should exist in the same namespace as the Coherence cluster. TBD&#8230;&#8203; ",
            "title": "Logging Configuration"
        },
        {
            "location": "/clusters/100_logging",
            "text": " The Coherence Operator allows Coherence cluster Pods to be configured with a Fluentd side-car container that will push Coherence logs to Elasticsearch. The configuration for Fluentd is in the logging.fluentd section of the spec. TBD&#8230;&#8203; ",
            "title": "Fluentd Logging Configuration"
        },
        {
            "location": "/developer/07_execution",
            "text": " NOTE: The Coherence Operator by default runs in and monitors a single namespace. This is different behaviour to v1.0 of the Coherence Operator. For more details see the Operator SDK document on Operator Scope . ",
            "title": "Namespaces"
        },
        {
            "location": "/developer/07_execution",
            "text": " To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >./hack/kill-local.sh ",
            "title": "Stopping the Local Operator"
        },
        {
            "location": "/developer/07_execution",
            "text": " During development running the Coherence Operator locally is by far the simplest option as it is faster and it also allows remote debugging if you are using a suitable IDE. To run a local copy of the operator that will connect to whatever you local kubernetes config is pointing to: <markup lang=\"bash\" >make run Stopping the Local Operator To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >./hack/kill-local.sh ",
            "title": "Running Locally"
        },
        {
            "location": "/developer/07_execution",
            "text": " After running the operator the CRDs can be removed from the k8s cluster by running the make target: <markup lang=\"bash\" >make uninstall-crds ",
            "title": "Clean-up"
        },
        {
            "location": "/developer/07_execution",
            "text": " The simplest and most reliable way to deploy the operator to K8s is to use the Helm chart. After building the operator the chart is created in the build/_output/helm-charts/coherence-operator directory. Using the Helm chart will ensure that all of the required RBAC rules are created when deploying to an environment where RBAC is enabled. The chart can be installed in the usual way with Helm <markup lang=\"bash\" >helm install --name operator \\ --namespace operator-test \\ build/_output/helm-charts/coherence-operator ",
            "title": "Deploying to Kubernetes"
        },
        {
            "location": "/developer/07_execution",
            "text": " There are two ways to run the Coherence Operator, either deployed into a k8s cluster or by using the Operator SDK to run it locally on your dev machine (assuming your dev machine has access to a k8s cluster such as Docker Desktop on MacOS). Namespaces NOTE: The Coherence Operator by default runs in and monitors a single namespace. This is different behaviour to v1.0 of the Coherence Operator. For more details see the Operator SDK document on Operator Scope . Running Locally During development running the Coherence Operator locally is by far the simplest option as it is faster and it also allows remote debugging if you are using a suitable IDE. To run a local copy of the operator that will connect to whatever you local kubernetes config is pointing to: <markup lang=\"bash\" >make run Stopping the Local Operator To stop the local operator just use CTRL-Z or CTRL-C. Sometimes processes can be left around even after exiting in this way. To make sure all of the processes are dead you can run the kill script: <markup lang=\"bash\" >./hack/kill-local.sh Clean-up After running the operator the CRDs can be removed from the k8s cluster by running the make target: <markup lang=\"bash\" >make uninstall-crds Deploying to Kubernetes The simplest and most reliable way to deploy the operator to K8s is to use the Helm chart. After building the operator the chart is created in the build/_output/helm-charts/coherence-operator directory. Using the Helm chart will ensure that all of the required RBAC rules are created when deploying to an environment where RBAC is enabled. The chart can be installed in the usual way with Helm <markup lang=\"bash\" >helm install --name operator \\ --namespace operator-test \\ build/_output/helm-charts/coherence-operator ",
            "title": "Running Coherence Operator Development"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The Coherence Operator is a Go based project built using the Operator SDK . ",
            "title": "preambule"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The following prerequisites are required to build and test the operator (the prerequisites to just run the operator are obviously a sub-set of these). operator-sdk version v0.9.0 git go version v1.12+. mercurial version 3.9+ docker version 17.03+. kubectl version v1.11.3+. Access to a Kubernetes v1.11.3+ cluster. Java 8+ JDK Maven version 3.5+ Access to a Maven repository containing Oracle Coherence 12.2.1.4 (for the exact GAV see the pom.xml file in the java/ directory) Optional: delve version 1.2.0+ (for local debugging with operator-sdk up local --enable-delve ). This project uses make for building, which should already be installed on most systems This project currently uses the Operator SDK v0.9.0 so make sure you install the correct version of the Operator SDK CLI. As stated above this project requires K8s v1.11.3+ so if using Docker on MacOS you need at least version 2.1.0.0 ",
            "title": "Development Prerequisites"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The project also contains a Java sub-project that is used to create Coherence utilities that the Operator relies on to work correctly with Coherence clusters that it is managing. This project was initially generated using the Operator SDK and this dictates the structure of the project which means that files and directories should not be moved arbitrarily. ",
            "title": "Project Structure"
        },
        {
            "location": "/developer/01_introduction",
            "text": " The following should not be moved: File Description bin/ scripts used in the Operator Docker image build/Dockerfile the Dockerfile used by the Operator SDK to build the Docker image cmd/manager/main.go The Operator main generated by the Operator SDK deploy/ Yaml files generated and maintained by the Operator SDK deploy/crds The CRD files generated and maintained by the Operator SDK helm-charts/ The Helm charts used by the Operator pkg/apis The API struct code generated by the Operator SDK and used to generate the CRD files pkg/controller The controller code generated by the Operator SDK watches.yaml The Helm Operator configuration generated by the Operator SDK local-watches.yaml The Helm Operator configuration used when running the operator locally ",
            "title": "Operator SDK Files"
        },
        {
            "location": "/developer/01_introduction",
            "text": " Development Prerequisites The following prerequisites are required to build and test the operator (the prerequisites to just run the operator are obviously a sub-set of these). operator-sdk version v0.9.0 git go version v1.12+. mercurial version 3.9+ docker version 17.03+. kubectl version v1.11.3+. Access to a Kubernetes v1.11.3+ cluster. Java 8+ JDK Maven version 3.5+ Access to a Maven repository containing Oracle Coherence 12.2.1.4 (for the exact GAV see the pom.xml file in the java/ directory) Optional: delve version 1.2.0+ (for local debugging with operator-sdk up local --enable-delve ). This project uses make for building, which should already be installed on most systems This project currently uses the Operator SDK v0.9.0 so make sure you install the correct version of the Operator SDK CLI. As stated above this project requires K8s v1.11.3+ so if using Docker on MacOS you need at least version 2.1.0.0 Project Structure The project also contains a Java sub-project that is used to create Coherence utilities that the Operator relies on to work correctly with Coherence clusters that it is managing. This project was initially generated using the Operator SDK and this dictates the structure of the project which means that files and directories should not be moved arbitrarily. Operator SDK Files The following should not be moved: File Description bin/ scripts used in the Operator Docker image build/Dockerfile the Dockerfile used by the Operator SDK to build the Docker image cmd/manager/main.go The Operator main generated by the Operator SDK deploy/ Yaml files generated and maintained by the Operator SDK deploy/crds The CRD files generated and maintained by the Operator SDK helm-charts/ The Helm charts used by the Operator pkg/apis The API struct code generated by the Operator SDK and used to generate the CRD files pkg/controller The controller code generated by the Operator SDK watches.yaml The Helm Operator configuration generated by the Operator SDK local-watches.yaml The Helm Operator configuration used when running the operator locally ",
            "title": "Coherence Operator Development"
        },
        {
            "location": "/about/01_overview",
            "text": " settings Install Installing and running the Coherence Operator. explore Guides Follow step-by-step guides to using the Coherence Operator to manage Coherence clusters. ",
            "title": "Get going"
        },
        {
            "location": "/about/01_overview",
            "text": " widgets Coherence Clusters In depth CoherenceCluster CRD documentation. build Developer Developer guide for building the Coherence Operator. ",
            "title": "In Depth"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " When using the implicit role configuration the replicas count is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 In this case a cluster will be created with a single implicit role named storage with a replica count of six. This will result in a StatefulSet with six Pods . ",
            "title": "Implicit Role Replicas"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " When using the explicit role configuration the replicas count is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The data role has a replica count of six The proxy role has a replic count of three ",
            "title": "Explicit Role Replicas"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " When using the explicit role configuration a value for replicas count can be set in the CoherenceCluster spec section that will be used as the default replicas value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 3 The default replicas value is set to six. The data and proxy roles do not have a replicas value so will use this default value and so will each have a StatefulSet with a replica count of six The web role has an explicit replicas value of three so will have three replicas in its StatefulSet ",
            "title": "Explicit Roles with Default Replicas"
        },
        {
            "location": "/clusters/040_replicas",
            "text": " The replica count for a role in a CoherenceCluster is set using the replicas field of a role spec. Implicit Role Replicas When using the implicit role configuration the replicas count is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 In this case a cluster will be created with a single implicit role named storage with a replica count of six. This will result in a StatefulSet with six Pods . Explicit Role Replicas When using the explicit role configuration the replicas count is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The data role has a replica count of six The proxy role has a replic count of three Explicit Roles with Default Replicas When using the explicit role configuration a value for replicas count can be set in the CoherenceCluster spec section that will be used as the default replicas value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 3 The default replicas value is set to six. The data and proxy roles do not have a replicas value so will use this default value and so will each have a StatefulSet with a replica count of six The web role has an explicit replicas value of three so will have three replicas in its StatefulSet ",
            "title": "Setting the Replica Count for a Role"
        },
        {
            "location": "/guides/080_management",
            "text": " Coherence clusters can be deployed with a ReST management endpoint enabled. ",
            "title": "preambule"
        },
        {
            "location": "/guides/080_management",
            "text": " TBD&#8230;&#8203; ",
            "title": "Deploying Coherence Clusters with Management"
        },
        {
            "location": "/install/01_introduction",
            "text": " The Coherence Operator is available as a Docker image oracle/coherence-operator:2.0.0-1909301023 that can easily be installed into a Kubernetes cluster. ",
            "title": "preambule"
        },
        {
            "location": "/install/01_introduction",
            "text": " There are two ways to install the Coherence Operator Using Helm using the Coherence Operator Helm chart Manually using Kubernetes APIs (e.g. kubectl ) ",
            "title": "Installation Options"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " The different configuration files commonly used by Coherence can be specified for a role in the role&#8217;s spec. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " There are three Coherence configuration files that can be set in a role&#8217;s specification: The Coherence Cache Configuration file The Coherence Operational Override file The POF Configuration file ",
            "title": "Coherence Config Files"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the implicit role configuration the cacheConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: cacheConfig: application-cache-config.xml In this case a cluster will be created with a single implicit role named storage where the coherence.cache.config system property and hence the cache configuration file used will be application-cache-config.xml ",
            "title": "Set the Cache Configuration for an Implicit Role"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the explicit role configuration the cacheConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data cacheConfig: data-cache-config.xml - role: proxy cacheConfig: proxy-cache-config.xml The data role will use the data-cache-config.xml cache configuration file The proxy role will use the proxy-cache-config.xml cache configuration file ",
            "title": "Set the Cache Configuration for Explicit Role"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the explicit role configuration a value for cacheConfig value can be set in the CoherenceCluster spec section that will be used as the default cacheConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: cacheConfig: application-cache-config.xml roles: - role: data - role: proxy - role: web cacheConfig: web-cache-config.xml The default cacheConfig value is set to application-cache-config.xml . The data and proxy roles do not have a cacheConfig value so will use this default value and will each have use the application-cache-config.xml file The web role has an explicit cacheConfig value of web-cache-config.xml so will use the web-cache-config.xml cache configuration file ",
            "title": "Set the Cache Configuration for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " The Coherence cache configuration file for a role in a CoherenceCluster is set using the cacheConfig field of a role spec. The value of this field will end up being passed to the Coherence JVM as the coherence.cache.config system property and will hence set the value of the cache configuration file used as described in the Coherence documentation. Set the Cache Configuration for an Implicit Role When using the implicit role configuration the cacheConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: cacheConfig: application-cache-config.xml In this case a cluster will be created with a single implicit role named storage where the coherence.cache.config system property and hence the cache configuration file used will be application-cache-config.xml Set the Cache Configuration for Explicit Role When using the explicit role configuration the cacheConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data cacheConfig: data-cache-config.xml - role: proxy cacheConfig: proxy-cache-config.xml The data role will use the data-cache-config.xml cache configuration file The proxy role will use the proxy-cache-config.xml cache configuration file Set the Cache Configuration for Explicit Roles with a Default When using the explicit role configuration a value for cacheConfig value can be set in the CoherenceCluster spec section that will be used as the default cacheConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: cacheConfig: application-cache-config.xml roles: - role: data - role: proxy - role: web cacheConfig: web-cache-config.xml The default cacheConfig value is set to application-cache-config.xml . The data and proxy roles do not have a cacheConfig value so will use this default value and will each have use the application-cache-config.xml file The web role has an explicit cacheConfig value of web-cache-config.xml so will use the web-cache-config.xml cache configuration file ",
            "title": "Setting the Coherence Cache Configuration File"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the implicit role configuration the overrideConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: overrideConfig: application-override.xml In this case a cluster will be created with a single implicit role named storage where the coherence.override system property and hence the operational override file used will be application-override.xml ",
            "title": "Set the Operational Override for an Implicit Role"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the explicit role configuration the overrideConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data overrideConfig: data-override.xml - role: proxy overrideConfig: proxy-override.xml The data role will use the data-override.xml operational override file The proxy role will use the proxy-override.xml operational override file ",
            "title": "Set the Operational Override for Explicit Role"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the explicit role configuration a value for overrideConfig value can be set in the CoherenceCluster spec section that will be used as the default overrideConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: overrideConfig: application-override.xml roles: - role: data - role: proxy - role: web overrideConfig: web-override.xml The default overrideConfig value is set to application-override.xml . The data and proxy roles do not have an overrideConfig value so will use this default value and will each have use the application-override.xml file The web role has an explicit overrideConfig value of web-override.xml so will use the web-override.xml operational override file ",
            "title": "Set the Operational Override for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " The Coherence operational override file for a role in a CoherenceCluster is set using the overrideConfig field of a role spec. The value of this field will end up being passed to the Coherence JVM as the coherence.override system property and will hence set the value of the operational override file used as described in the Coherence documentation. Set the Operational Override for an Implicit Role When using the implicit role configuration the overrideConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: overrideConfig: application-override.xml In this case a cluster will be created with a single implicit role named storage where the coherence.override system property and hence the operational override file used will be application-override.xml Set the Operational Override for Explicit Role When using the explicit role configuration the overrideConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data overrideConfig: data-override.xml - role: proxy overrideConfig: proxy-override.xml The data role will use the data-override.xml operational override file The proxy role will use the proxy-override.xml operational override file Set the Operational Override for Explicit Roles with a Default When using the explicit role configuration a value for overrideConfig value can be set in the CoherenceCluster spec section that will be used as the default overrideConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: overrideConfig: application-override.xml roles: - role: data - role: proxy - role: web overrideConfig: web-override.xml The default overrideConfig value is set to application-override.xml . The data and proxy roles do not have an overrideConfig value so will use this default value and will each have use the application-override.xml file The web role has an explicit overrideConfig value of web-override.xml so will use the web-override.xml operational override file ",
            "title": "Setting the Coherence Operational Override File"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the implicit role configuration the pofConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: pofConfig: application-pof-config.xml In this case a cluster will be created with a single implicit role named storage where the coherence.pof.config system property and hence the POF configuration file used will be application-pof-config.xml ",
            "title": "Set the Cache Configuration for an Implicit Role"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the explicit role configuration the pofConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data pofConfig: data-pof-config.xml - role: proxy pofConfig: proxy-pof-config.xml The data role will use the data-pof-config.xml POF configuration file The proxy role will use the proxy-pof-config.xml POF configuration file ",
            "title": "Set the Cache Configuration for Explicit Role"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " When using the explicit role configuration a value for pofConfig value can be set in the CoherenceCluster spec section that will be used as the default pofConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: pofConfig: application-pof-config.xml roles: - role: data - role: proxy - role: web pofConfig: web-pof-config.xml The default pofConfig value is set to application-pof-config.xml . The data and proxy roles do not have a pofConfig value so will use this default value and will each have use the application-pof-config.xml file The web role has an explicit pofConfig value of web-pof-config.xml so will use the web-pof-config.xml POF configuration file ",
            "title": "Set the Cache Configuration for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/050_config_files",
            "text": " The Coherence POF configuration file for a role in a CoherenceCluster is set using the pofConfig field of a role spec. The value of this field will end up being passed to the Coherence JVM as the coherence.pof.config system property and will hence set the value of the POF configuration file used as described in the Coherence documentation. Set the Cache Configuration for an Implicit Role When using the implicit role configuration the pofConfig value is set directly in the CoherenceCluster spec section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: pofConfig: application-pof-config.xml In this case a cluster will be created with a single implicit role named storage where the coherence.pof.config system property and hence the POF configuration file used will be application-pof-config.xml Set the Cache Configuration for Explicit Role When using the explicit role configuration the pofConfig value is set for each role in the CoherenceCluster spec roles list. For example to create cluster with two explicit roles, data and proxy : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data pofConfig: data-pof-config.xml - role: proxy pofConfig: proxy-pof-config.xml The data role will use the data-pof-config.xml POF configuration file The proxy role will use the proxy-pof-config.xml POF configuration file Set the Cache Configuration for Explicit Roles with a Default When using the explicit role configuration a value for pofConfig value can be set in the CoherenceCluster spec section that will be used as the default pofConfig value for any role in the roles list that does not explicitly specify a value. For example to create cluster with three explicit roles, data and proxy and web : <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: pofConfig: application-pof-config.xml roles: - role: data - role: proxy - role: web pofConfig: web-pof-config.xml The default pofConfig value is set to application-pof-config.xml . The data and proxy roles do not have a pofConfig value so will use this default value and will each have use the application-pof-config.xml file The web role has an explicit pofConfig value of web-pof-config.xml so will use the web-pof-config.xml POF configuration file ",
            "title": "Setting the POF Configuration File"
        },
        {
            "location": "/about/03_kubernetes",
            "text": " For development and testing of the Coherence Operator it&#8217;s often convenient to run Kubernetes on your desktop. Some ways to do this are: Kubernetes support in Docker for Desktop Kind Kubernetes Minikube ",
            "title": "preambule"
        },
        {
            "location": "/about/03_kubernetes",
            "text": " Install Docker for Mac or Docker for Windows . Starting with version 18.06 Docker for Desktop includes Kubernetes support. ",
            "title": "Install"
        },
        {
            "location": "/about/03_kubernetes",
            "text": " Enable Kubernetes Support for Mac or Kubernetes Support for Windows . Once Kubernetes installation is complete, make sure you have your context set correctly to use docker-for-desktop. <markup lang=\"bash\" title=\"Make sure K8s context is set to docker-for-desktop\" >kubectl config get-contexts kubectl config use-context docker-for-desktop kubectl cluster-info kubectl version --short kubectl get nodes ",
            "title": "Enable Kubernetes Support"
        },
        {
            "location": "/about/03_kubernetes",
            "text": " Install Install Docker for Mac or Docker for Windows . Starting with version 18.06 Docker for Desktop includes Kubernetes support. Enable Kubernetes Support Enable Kubernetes Support for Mac or Kubernetes Support for Windows . Once Kubernetes installation is complete, make sure you have your context set correctly to use docker-for-desktop. <markup lang=\"bash\" title=\"Make sure K8s context is set to docker-for-desktop\" >kubectl config get-contexts kubectl config use-context docker-for-desktop kubectl cluster-info kubectl version --short kubectl get nodes ",
            "title": "Docker for Desktop."
        },
        {
            "location": "/about/03_kubernetes",
            "text": " If trying to use Kind to run the Coherence Operator using locally built images these images need to be added to the Kubernetes cluster using the Kind CLI because the local images will obviously not be in a repository that the nodes can pull from. Although a Kind cluster is running in Docker it does not appear to have access to any local Docker images so all images either need to be pull-able or loaded via the Kind CLI. For example if the Operator has been built with make all there will be the following local images <markup lang=\"bash\" >docker images --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/operator-test-image 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci-utils These images can be added to the Kind cluster with the commands: <markup lang=\"bash\" >kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci-utils kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/operator-test-image:2.0.0-ci ",
            "title": "A Word About Kind and Docker Images"
        },
        {
            "location": "/about/03_kubernetes",
            "text": " Install Kind as described in the Kind Quick Start To create a Kubernetes three node cluster in Kind you need a configuration file <markup lang=\"yaml\" title=\"kind-config.yaml\" >kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 nodes: - role: control-plane - role: worker - role: worker - role: worker Then create a Kind Kubernetes cluster with the following command <markup lang=\"bash\" >kind create cluster --config kind-config.yaml After a short while (depending on how long images take to download) there should be a Kubernetes cluster with a master and three worker nodes running in Docker containers. <markup lang=\"bash\" >docker ps d790d6b779ff kindest/node:v1.15.3 \"/usr/local/bin/entr…\" 23 hours ago Up 23 hours kind-worker2 a096c8bf0c1a kindest/node:v1.15.3 \"/usr/local/bin/entr…\" 23 hours ago Up 23 hours kind-worker3 4c01d94c29b7 kindest/node:v1.15.3 \"/usr/local/bin/entr…\" 23 hours ago Up 23 hours 56603/tcp, 127.0.0.1:56603-&gt;6443/tcp kind-control-plane 8f62284be151 kindest/node:v1.15.3 \"/usr/local/bin/entr…\" 23 hours ago Up 23 hours kind-worker As described in the Kind documentation now export KUBECONFIG for the Kind cluster <markup lang=\"bash\" >export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\" To be able to use this cluster with the Operator Helm chart Helm will need to be initialised. <markup lang=\"bash\" >helm init The Kind cluster has RBAC enabled so Helm&#8217;s Tiller will now need to be patched with a role: <markup lang=\"bash\" >kubectl create serviceaccount \\ --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule \\ --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system \\ tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' A Word About Kind and Docker Images If trying to use Kind to run the Coherence Operator using locally built images these images need to be added to the Kubernetes cluster using the Kind CLI because the local images will obviously not be in a repository that the nodes can pull from. Although a Kind cluster is running in Docker it does not appear to have access to any local Docker images so all images either need to be pull-able or loaded via the Kind CLI. For example if the Operator has been built with make all there will be the following local images <markup lang=\"bash\" >docker images --format \"table {{.Repository}}\\t{{.Tag}}\" REPOSITORY TAG iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/operator-test-image 2.0.0-ci iad.ocir.io/odx-stateservice/test/oracle/coherence-operator 2.0.0-ci-utils These images can be added to the Kind cluster with the commands: <markup lang=\"bash\" >kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/coherence-operator:2.0.0-ci-utils kind load docker-image iad.ocir.io/odx-stateservice/test/oracle/operator-test-image:2.0.0-ci ",
            "title": "Kind"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " Annotations can be added to the Pods of a role in a CoherenceCluster . ",
            "title": "preambule"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " When creating a CoherenceCluster with a single implicit role annotations can be defined at the spec level. Annotations are defined as a map of string key value pairs, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back The implicit role will have the annotation acme.com/layer : back This will result in a StatefulSet for the role with the annotation added to the PodSpec . <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster-storage spec: replicas: 3 selector: matchLabels: coherenceDeployment: test-cluster-storage component: coherencePod serviceName: test-cluster-storage template: metadata: annotations: acme.com/layer: back The annotation acme.com/layer: back has been applied to the StatefulSet Pod template. ",
            "title": "Configure Pod Annotations for the Implicit Role"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data annotations: acme.com/layer: back - role: proxy annotations: acme.com/layer: front The data role will have the annotation acme.com/layer: back The proxy role will have the annotation acme.com/layer: front ",
            "title": "Configure Pod Annotations for Explicit Roles"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined as defaults applied to all roles and also for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back acme.com/app: orders roles: - role: data - role: proxy annotations: acme.com/state: none - role: web annotations: acme.com/three: none acme.com/layer: front There are two default annotations acme.com/layer : back and acme.com/app : orders that will apply to all Pods in all roles unless specifically overridden. The data role has no other annotations defined so will just have the default annotations acme.com/layer : back and acme.com/app : orders The proxy role specified an annotation acme.com/state : none so will have this annotation as well as the default annotations acme.com/layer : back and acme.com/app : orders The web role specifies the acme.com/three: none annotation and also overrides the acme.com/layer annotation with the value front so it will have three annotations, acme.com/three: none , acme.com/layer : front and acme.com/app : orders ",
            "title": "Configure Pod Annotations for Explicit Roles With Defaults"
        },
        {
            "location": "/clusters/120_annotations",
            "text": " Custom annotations can be added to the spec of a role which will then be added to all Pods for that role created by the Coherence Operator. Configure Pod Annotations for the Implicit Role When creating a CoherenceCluster with a single implicit role annotations can be defined at the spec level. Annotations are defined as a map of string key value pairs, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back The implicit role will have the annotation acme.com/layer : back This will result in a StatefulSet for the role with the annotation added to the PodSpec . <markup lang=\"yaml\" >apiVersion: apps/v1 kind: StatefulSet metadata: name: test-cluster-storage spec: replicas: 3 selector: matchLabels: coherenceDeployment: test-cluster-storage component: coherencePod serviceName: test-cluster-storage template: metadata: annotations: acme.com/layer: back The annotation acme.com/layer: back has been applied to the StatefulSet Pod template. Configure Pod Annotations for Explicit Roles When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data annotations: acme.com/layer: back - role: proxy annotations: acme.com/layer: front The data role will have the annotation acme.com/layer: back The proxy role will have the annotation acme.com/layer: front Configure Pod Annotations for Explicit Roles With Defaults When creating a CoherenceCluster with explicit roles in the roles list annotations can be defined as defaults applied to all roles and also for each role, for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: annotations: acme.com/layer: back acme.com/app: orders roles: - role: data - role: proxy annotations: acme.com/state: none - role: web annotations: acme.com/three: none acme.com/layer: front There are two default annotations acme.com/layer : back and acme.com/app : orders that will apply to all Pods in all roles unless specifically overridden. The data role has no other annotations defined so will just have the default annotations acme.com/layer : back and acme.com/app : orders The proxy role specified an annotation acme.com/state : none so will have this annotation as well as the default annotations acme.com/layer : back and acme.com/app : orders The web role specifies the acme.com/three: none annotation and also overrides the acme.com/layer annotation with the value front so it will have three annotations, acme.com/three: none , acme.com/layer : front and acme.com/app : orders ",
            "title": "Pod Annotations"
        },
        {
            "location": "/clusters/080_environment_variables",
            "text": " It is possible to pass arbitrary environment variables to the Pods that are created for a Coherence cluster. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/080_environment_variables",
            "text": " If configuring a single implicit role environment variables are set in the spec.env section; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" - name: BAR value: \"bar-val\" The FOO environment variable with a value of foo-val will be passed to the coherence container in the Pods created for the implicit role. The BAR environment variable with a value of bar-val will be passed to the coherence container in the Pods created for the implicit role. ",
            "title": "Environment Variables in the Implicit Role"
        },
        {
            "location": "/clusters/080_environment_variables",
            "text": " When configuring one or more explicit roles in the roles section of the spec environment variables can be configured for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data env: - name: FOO value: \"foo-val\" - role: proxy env: - name: BAR value: \"bar-val\" All Pods created for the data role will have the FOO environment variable set to foo-val All Pods created for the proxy role will have the BAR environment variable set to bar-val ",
            "title": "Environment Variables in Explicit Roles"
        },
        {
            "location": "/clusters/080_environment_variables",
            "text": " When configuring one or more explicit roles it is also possible to configure environment variables at the defaults level. These environment variables will be shared by all Pods in all roles unless specifically overridden for a role. An environment variable is only overridden in a role by declaring a role level environment variable with the same name. When creating the Pods configuration the Coherence Operator will merge the list of default environment variables with the role&#8217;s list of environment variables. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" roles: - role: data - role: proxy env: - name: BAR value: \"bar-val\" - role: web env: - name: FOO value: \"foo-web\" - name: BAR value: \"bar-web\" The data role does not have any environment variables configured so it will just inherit the FOO=foo-val environment variable from the defaults. The proxy role has the BAR=bar-val environment variables configured and will also inherit the FOO=foo-val environment variable from the defaults. The web role has will override the FOO environment variable from the default with FOO=foo-web . It also has its own BAR=bar-web environment variable. ",
            "title": "Environment Variables in Explicit Roles With Defaults"
        },
        {
            "location": "/clusters/080_environment_variables",
            "text": " Environment variables can be configured in a CoherenceCluster and will be passed through to the Coherence Pods created for the roles in the cluster. Environment variables are configured in the env field of the spec. The format for setting environment variables is exactly the same as when configuring them in a Kubernetes Container . Environment Variables in the Implicit Role If configuring a single implicit role environment variables are set in the spec.env section; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" - name: BAR value: \"bar-val\" The FOO environment variable with a value of foo-val will be passed to the coherence container in the Pods created for the implicit role. The BAR environment variable with a value of bar-val will be passed to the coherence container in the Pods created for the implicit role. Environment Variables in Explicit Roles When configuring one or more explicit roles in the roles section of the spec environment variables can be configured for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data env: - name: FOO value: \"foo-val\" - role: proxy env: - name: BAR value: \"bar-val\" All Pods created for the data role will have the FOO environment variable set to foo-val All Pods created for the proxy role will have the BAR environment variable set to bar-val Environment Variables in Explicit Roles With Defaults When configuring one or more explicit roles it is also possible to configure environment variables at the defaults level. These environment variables will be shared by all Pods in all roles unless specifically overridden for a role. An environment variable is only overridden in a role by declaring a role level environment variable with the same name. When creating the Pods configuration the Coherence Operator will merge the list of default environment variables with the role&#8217;s list of environment variables. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: env: - name: FOO value: \"foo-val\" roles: - role: data - role: proxy env: - name: BAR value: \"bar-val\" - role: web env: - name: FOO value: \"foo-web\" - name: BAR value: \"bar-web\" The data role does not have any environment variables configured so it will just inherit the FOO=foo-val environment variable from the defaults. The proxy role has the BAR=bar-val environment variables configured and will also inherit the FOO=foo-val environment variable from the defaults. The web role has will override the FOO environment variable from the default with FOO=foo-web . It also has its own BAR=bar-web environment variable. ",
            "title": "Environment Variables"
        },
        {
            "location": "/guides/090_metrics",
            "text": " Coherence clusters can be deployed with a metrics endpoint enabled that can be scraped by common metrics applications such as Prometheus. ",
            "title": "preambule"
        },
        {
            "location": "/guides/090_metrics",
            "text": " To enable Prometheus, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ coherence/coherence-operator Set &lt;namespace&gt; to the Kubernetes namespace that the Coherence Operator should be installed into. After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s The Coherence Operator Pod The Grafana Pod The Prometheus Pod The demo install of Prometheus in the Operator configures Prometheus to use service monitors to work out which Pods to scrape metrics from. A ServiceMonitor in Prometheus will scrape from a port defined in a Kubernetes Service from all Pods that match that service&#8217;s selector. ",
            "title": "1. Install the Coherence Operator with Prometheus"
        },
        {
            "location": "/guides/090_metrics",
            "text": " Now that Prometheus is running Coherence clusters can be created that expose metrics on a port on each Pod and also deploy a Service to expose the metrics that Prometheus can use. Deploy a simple metrics enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: storage replicas: 2 metrics: enabled: true ports: - name: metrics port: 9612 This cluster will have a single role called storage The cluster will have two replicas ( Pods ) The Coherence Pod spec contains a port spec for metrics named metric so this needs to be exposed as a service by specifying the metrics port in the role&#8217;s ports list The port must be set as 9612 which is the port that Coherence will expose metrics on The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f metrics-cluster.yaml The Coherence Operator will see the new CoherenceCluster resource and create the cluster with two Pods . If kubectl get pods -n &lt;namespace&gt; is run again it should now look something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s test-cluster-storage-0 1/1 Running 0 70s test-cluster-storage-1 1/1 Running 0 70s Pod one of the Coherence cluster. Pod two of the Coherence cluster. If the services are listed for the namespace: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get svc The list of services will look something like this. <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE operator-grafana ClusterIP 10.104.251.51 &lt;none&gt; 80/TCP 31m operator-kube-state-metrics ClusterIP 10.110.18.78 &lt;none&gt; 8080/TCP 31m operator-prometheus-node-exporter ClusterIP 10.102.181.6 &lt;none&gt; 9100/TCP 31m operator-prometheusoperato-operator ClusterIP 10.107.59.229 &lt;none&gt; 8080/TCP 31m operator-prometheusoperato-prometheus ClusterIP 10.99.208.18 &lt;none&gt; 9090/TCP 31m prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 31m test-cluster-storage-headless ClusterIP None &lt;none&gt; 30000/TCP 16m test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 16m test-cluster-wka ClusterIP None &lt;none&gt; 30000/TCP 16m One of the services will be the service exposing the Coherence metrics. The service name is typically in the format &lt;cluster-name&gt;-&lt;role-name&gt;-&lt;port-name&gt; The Prometheus ServiceMonitor installed by the Coherence Operator is configured to look for services with the label component=coherence-service-metrics . When ports are exposed in a CoherenceCluster , as has been done here for metrics, the service created will have a label of the format component=coherence-service-&lt;port-name&gt; , so in this case the test-cluster-storage-metrics service above will have the label component=coherence-service-metrics . The labels for the service can be displayed: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt;&gt; get svc/test-cluster-storage-metrics --label-columns=component <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE COMPONENT test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 26m coherence-service-metrics Which shows that the service does indeed have the required label. ",
            "title": "2. Install a Coherence Cluster with Metrics Enabled"
        },
        {
            "location": "/guides/090_metrics",
            "text": " Now that Prometheus is running and is able to scrape metrics from the Coherence cluster it should be possible to access those metrics in Prometheus. First find the Prometheus Pod name using kubectl <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name Using the Pod name use kubectl to create a port forward session to the Prometheus Pod so that the Prometheus API on port 9090 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name) \\ 9090:9090 It is now possible to access the Prometheus API on localhost port 9090. This can be used to directly retrieve Coherence metrics using curl , for example to obtain the cluster size metric: <markup lang=\"bash\" >curl -w '' -X GET http://127.0.0.1:9090/api/v1/query?query=vendor:coherence_cluster_size It is also possible to browse directly to the Prometheus web UI at http://127.0.0.1:9090 ",
            "title": "3. Access Prometheus"
        },
        {
            "location": "/guides/090_metrics",
            "text": " By default when the Coherence Operator configured to install Prometheus the Prometheus Operator also install a Grafana Pod and the Coherence Operator imports into Grafana a number of custom dashboards for displaying Coherence metrics. Grafana can be accessed by using port forwarding in the same way that was done for Prometheus First find the Grafana Pod : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name Using the Pod name use kubectl to create a port forward session to the Grafana Pod so that the Grafana API on port 3000 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name) \\ 3000:3000 The custom Coherence dashboards can be accessed by pointing a browser to http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main The Grafana credentials are username admin password prom-operator ",
            "title": "3. Access Grafana"
        },
        {
            "location": "/guides/090_metrics",
            "text": " After running the demo above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f metrics-cluster.yaml The Coherence Operator, along with Prometheus and Grafana can be removed using Helm: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "4. Cleaning Up"
        },
        {
            "location": "/guides/090_metrics",
            "text": " Note: Use of metrics is available only when using the operator with clusters running Coherence 12.2.1.4 or later version. The Coherence Operator can be installed with a demo Prometheus installation using embedded Prometheus Operator and Grafana Helm charts. This Prometheus deployment is not intended for production use but is useful for development, testing and demo purposes. 1. Install the Coherence Operator with Prometheus To enable Prometheus, add the following options to the Operator Helm install command: <markup lang=\"bash\" >--set prometheusoperator.enabled=true --set prometheusoperator.prometheusOperator.createCustomResource=false A more complete helm install command to enable Prometheus is as follows: <markup lang=\"bash\" >helm install \\ --namespace &lt;namespace&gt; \\ --name coherence-operator \\ --set prometheusoperator.enabled=true \\ --set prometheusoperator.prometheusOperator.createCustomResource=false \\ coherence/coherence-operator Set &lt;namespace&gt; to the Kubernetes namespace that the Coherence Operator should be installed into. After the installation completes, list the pods in the namespace that the Operator was installed into: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pods The results returned should look something like the following: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s The Coherence Operator Pod The Grafana Pod The Prometheus Pod The demo install of Prometheus in the Operator configures Prometheus to use service monitors to work out which Pods to scrape metrics from. A ServiceMonitor in Prometheus will scrape from a port defined in a Kubernetes Service from all Pods that match that service&#8217;s selector. 2. Install a Coherence Cluster with Metrics Enabled Now that Prometheus is running Coherence clusters can be created that expose metrics on a port on each Pod and also deploy a Service to expose the metrics that Prometheus can use. Deploy a simple metrics enabled CoherenceCluster resource with a single role like this: <markup lang=\"yaml\" title=\"metrics-cluster.yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: storage replicas: 2 metrics: enabled: true ports: - name: metrics port: 9612 This cluster will have a single role called storage The cluster will have two replicas ( Pods ) The Coherence Pod spec contains a port spec for metrics named metric so this needs to be exposed as a service by specifying the metrics port in the role&#8217;s ports list The port must be set as 9612 which is the port that Coherence will expose metrics on The yaml above can be installed into Kubernetes using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f metrics-cluster.yaml The Coherence Operator will see the new CoherenceCluster resource and create the cluster with two Pods . If kubectl get pods -n &lt;namespace&gt; is run again it should now look something like this: <markup lang=\"bash\" >NAME READY STATUS RESTARTS AGE operator-coherence-operator-5d779ffc7-7xz7j 1/1 Running 0 53s operator-grafana-9d7fc9486-46zb7 2/2 Running 0 53s operator-kube-state-metrics-7b4fcc5b74-ljdf8 1/1 Running 0 53s operator-prometheus-node-exporter-kwdr7 1/1 Running 0 53s operator-prometheusoperato-operator-77c784b8c5-v4bfz 1/1 Running 0 53s prometheus-operator-prometheusoperato-prometheus-0 3/3 Running 2 38s test-cluster-storage-0 1/1 Running 0 70s test-cluster-storage-1 1/1 Running 0 70s Pod one of the Coherence cluster. Pod two of the Coherence cluster. If the services are listed for the namespace: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get svc The list of services will look something like this. <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE operator-grafana ClusterIP 10.104.251.51 &lt;none&gt; 80/TCP 31m operator-kube-state-metrics ClusterIP 10.110.18.78 &lt;none&gt; 8080/TCP 31m operator-prometheus-node-exporter ClusterIP 10.102.181.6 &lt;none&gt; 9100/TCP 31m operator-prometheusoperato-operator ClusterIP 10.107.59.229 &lt;none&gt; 8080/TCP 31m operator-prometheusoperato-prometheus ClusterIP 10.99.208.18 &lt;none&gt; 9090/TCP 31m prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 31m test-cluster-storage-headless ClusterIP None &lt;none&gt; 30000/TCP 16m test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 16m test-cluster-wka ClusterIP None &lt;none&gt; 30000/TCP 16m One of the services will be the service exposing the Coherence metrics. The service name is typically in the format &lt;cluster-name&gt;-&lt;role-name&gt;-&lt;port-name&gt; The Prometheus ServiceMonitor installed by the Coherence Operator is configured to look for services with the label component=coherence-service-metrics . When ports are exposed in a CoherenceCluster , as has been done here for metrics, the service created will have a label of the format component=coherence-service-&lt;port-name&gt; , so in this case the test-cluster-storage-metrics service above will have the label component=coherence-service-metrics . The labels for the service can be displayed: <markup lang=\"bash\" >kubectl -n &lt;namespace&gt;&gt; get svc/test-cluster-storage-metrics --label-columns=component <markup lang=\"bash\" >NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE COMPONENT test-cluster-storage-metrics ClusterIP 10.109.201.211 &lt;none&gt; 9612/TCP 26m coherence-service-metrics Which shows that the service does indeed have the required label. 3. Access Prometheus Now that Prometheus is running and is able to scrape metrics from the Coherence cluster it should be possible to access those metrics in Prometheus. First find the Prometheus Pod name using kubectl <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name Using the Pod name use kubectl to create a port forward session to the Prometheus Pod so that the Prometheus API on port 9090 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=prometheus -o name) \\ 9090:9090 It is now possible to access the Prometheus API on localhost port 9090. This can be used to directly retrieve Coherence metrics using curl , for example to obtain the cluster size metric: <markup lang=\"bash\" >curl -w '' -X GET http://127.0.0.1:9090/api/v1/query?query=vendor:coherence_cluster_size It is also possible to browse directly to the Prometheus web UI at http://127.0.0.1:9090 3. Access Grafana By default when the Coherence Operator configured to install Prometheus the Prometheus Operator also install a Grafana Pod and the Coherence Operator imports into Grafana a number of custom dashboards for displaying Coherence metrics. Grafana can be accessed by using port forwarding in the same way that was done for Prometheus First find the Grafana Pod : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name Using the Pod name use kubectl to create a port forward session to the Grafana Pod so that the Grafana API on port 3000 in the Pod can be accessed from the local host. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; port-forward \\ $(kubectl -n &lt;namespace&gt; get pod -l app=grafana -o name) \\ 3000:3000 The custom Coherence dashboards can be accessed by pointing a browser to http://127.0.0.1:3000/d/coh-main/coherence-dashboard-main The Grafana credentials are username admin password prom-operator 4. Cleaning Up After running the demo above the Coherence cluster can be removed using kubectl : <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; delete -f metrics-cluster.yaml The Coherence Operator, along with Prometheus and Grafana can be removed using Helm: <markup lang=\"bash\" >helm delete --purge coherence-operator ",
            "title": "Deploying Coherence Clusters with Metrics"
        },
        {
            "location": "/install/05_pre_release_versions",
            "text": " Pre-release version of the Coherence Operator are made available from time to time. ",
            "title": "preambule"
        },
        {
            "location": "/install/05_pre_release_versions",
            "text": " Not all pre-release images are pushed to public repositories such as Docker Hub. Consequently when installing those versions of the Coherence Operator credentials and Kubernetes pull secrets will be required. For example to access an image in the iad.ocir.io/odx-stateservice repository you would need to have your own credentials for that repository so that a secret can be created. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; \\ create secret docker-registry coherence-operator-secret \\ --docker-server=$DOCKER_REPO \\ --docker-username=$DOCKER_USERNAME \\ --docker-password=$DOCKER_PASSWORD \\ --docker-email=$DOCKER_EMAIL Replace &lt;namespace&gt; with the Kubernetes namespace that the Coherence Operator will be installed into. In this example the name of the secret to be created is coherence-operator-secret Replace $DOCKER_REPO with the name of the Docker repository that the images are to be pulled from. Replace $DOCKER_USERNAME with your username for that repository. Replace $DOCKER_PASSWORD with your password for that repository. Replace $DOCKER_EMAIL with your email (or even a fake email). See the Kubernetes documentation on pull secrets for more details. Once a secret has been created in the namespace the Coherence Operator can be installed with an extra value parameter to specify the secret to use: <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 2.0.0-1909130555 \\ --namespace &lt;namespace&gt; \\ --set imagePullSecrets=coherence-operator-secret \\ --name coherence-operator Set the pull secret to use to the same name that was created above. ",
            "title": "Accessing Pre-Release Coherence Operator Docker Images"
        },
        {
            "location": "/install/05_pre_release_versions",
            "text": " Pre-release versions of the Coherence Operator are not guaranteed to be bug free and should not be used for production use. Pre-release versions of the Helm chart and Docker images may be removed and hence made unavailable without notice. APIs and CRD specifications are not guaranteed to remain stable or backwards compatible between pre-release versions. To access pre-release versions of the Helm chart add the unstable chart repository. <markup lang=\"bash\" >helm repo add coherence-unstable https://oracle.github.io/coherence-operator/charts-unstable helm repo update To list all of the available Coherence Operator chart versions: <markup lang=\"bash\" >helm search coherence-operator -l The -l parameter shows all versions as opposed to just the latest versions if it was omitted. A specific pre-release version of the Helm chart can be installed using the --version argument, for example to use version 2.0.0-alpha1 : <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 2.0.0-alpha1 \\ --namespace &lt;namespace&gt; \\ --name coherence-operator The --version argument is used to specify the exact version of the chart The optional --namespace parameter to specify which namespace to install the operator into, if omitted then Helm will install into whichever is currently the default namespace for your Kubernetes configuration. When using pre-release versions of the Helm chart it is always advisable to install a specific version otherwise Helm will try to work out the latest version in the pre-release repo and as pre-release version numbers are not strictly sem-ver compliant this may be unreliable. Accessing Pre-Release Coherence Operator Docker Images Not all pre-release images are pushed to public repositories such as Docker Hub. Consequently when installing those versions of the Coherence Operator credentials and Kubernetes pull secrets will be required. For example to access an image in the iad.ocir.io/odx-stateservice repository you would need to have your own credentials for that repository so that a secret can be created. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; \\ create secret docker-registry coherence-operator-secret \\ --docker-server=$DOCKER_REPO \\ --docker-username=$DOCKER_USERNAME \\ --docker-password=$DOCKER_PASSWORD \\ --docker-email=$DOCKER_EMAIL Replace &lt;namespace&gt; with the Kubernetes namespace that the Coherence Operator will be installed into. In this example the name of the secret to be created is coherence-operator-secret Replace $DOCKER_REPO with the name of the Docker repository that the images are to be pulled from. Replace $DOCKER_USERNAME with your username for that repository. Replace $DOCKER_PASSWORD with your password for that repository. Replace $DOCKER_EMAIL with your email (or even a fake email). See the Kubernetes documentation on pull secrets for more details. Once a secret has been created in the namespace the Coherence Operator can be installed with an extra value parameter to specify the secret to use: <markup lang=\"bash\" >helm install coherence-unstable/coherence-operator \\ --version 2.0.0-1909130555 \\ --namespace &lt;namespace&gt; \\ --set imagePullSecrets=coherence-operator-secret \\ --name coherence-operator Set the pull secret to use to the same name that was created above. ",
            "title": "Accessing Pre-Release Versions"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " The application image is the Docker image containing the .jar files and configuration files of the Coherence application that should be deployed in the Coherence cluster. For more information see the Deploying Coherence Applications guide. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " When using the implicit role configuration the application image to use is set directly in the CoherenceCluster spec images.userArtifacts.image section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 The acme/orders-data/1.0.0 will be used to add additional .jar files and configuration files to the classpath of the Coherence container in the implicit storage role&#8217;s Pods ",
            "title": "Setting the Application Image for the Implicit Role"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " When using the explicit roles in a CoherenceCluster roles list the application image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: userArtifacts: image: acme/orders-data/1.0.0 - role: proxy images: userArtifacts: image: acme/orders-proxy/1.0.0 The data role Pods will use the application image acme/orders-data/1.0.0 The proxy role Pods will use the application image acme/orders-proxy/1.0.0 ",
            "title": "Setting the Application Image for Explicit Roles"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " When using the explicit roles in a CoherenceCluster roles list the application image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 roles: - role: data - role: proxy The data and the proxy roles will both use the application image acme/orders-data/1.0.0 <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 roles: - role: data - role: proxy - role: web images: userArtifacts: image: acme/orders-front-end/1.0.0 The data and the proxy roles will both use the application image acme/orders-data/1.0.0 The web role will use the application image acme/orders-web/1.0.0 ",
            "title": "Setting the Application Image for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " Whilst the Coherence Operator makes it simple to deploy and manage a Coherence cluster in Kubernetes in the majority of use cases there will be a requirement for application code to be deployed and run in the Coherence JVMs. This application code and any application configuration files are supplied as a separate image. This image is loaded as an init-container by the Coherence Pods and the relevant .jar files and configuration files from this image are added to the Coherence JVM&#8217;s classpath. As well as setting the image name it is also sometimes useful to set the application image&#8217;s image pull policy . Setting the Application Image for the Implicit Role When using the implicit role configuration the application image to use is set directly in the CoherenceCluster spec images.userArtifacts.image section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 The acme/orders-data/1.0.0 will be used to add additional .jar files and configuration files to the classpath of the Coherence container in the implicit storage role&#8217;s Pods Setting the Application Image for Explicit Roles When using the explicit roles in a CoherenceCluster roles list the application image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: userArtifacts: image: acme/orders-data/1.0.0 - role: proxy images: userArtifacts: image: acme/orders-proxy/1.0.0 The data role Pods will use the application image acme/orders-data/1.0.0 The proxy role Pods will use the application image acme/orders-proxy/1.0.0 Setting the Application Image for Explicit Roles with a Default When using the explicit roles in a CoherenceCluster roles list the application image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 roles: - role: data - role: proxy The data and the proxy roles will both use the application image acme/orders-data/1.0.0 <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 roles: - role: data - role: proxy - role: web images: userArtifacts: image: acme/orders-front-end/1.0.0 The data and the proxy roles will both use the application image acme/orders-data/1.0.0 The web role will use the application image acme/orders-web/1.0.0 ",
            "title": "Setting the Application Image"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always ",
            "title": "Setting the Image Pull Policy for the Implicit Role"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: userArtifacts: image: acme/orders-data/1.0.0 imagePullPolicy: Always - role: proxy images: userArtifacts: image: acme/orders-proxy/1.0.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: imagePullPolicy: Always roles: - role: data images: userArtifacts: image: acme/orders-data/1.0.0 - role: proxy images: userArtifacts: image: acme/orders-proxy/1.0.1 - role: web images: userArtifacts: image: acme/orders-front-end/1.0.1 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles with Default"
        },
        {
            "location": "/clusters/065_application_image",
            "text": " The image pull policy controls when (and if) Kubernetes will pull the application image onto the node where the Coherence Pods are being schedules. See Kubernetes imagePullPolicy for more information. The Kubernetes default pull policy is IfNotPresent unless the image tag is :latest in which case the default policy is Always . The IfNotPresent policy causes the Kubelet to skip pulling an image if it already exists. Note that you should avoid using the :latest tag, see Kubernetes Best Practices for Configuration for more information. The application image&#8217;s pull policy is set using the imagePullPolicy field in the spec.images.coherence section. Setting the Image Pull Policy for the Implicit Role To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: image: acme/orders-data/1.0.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always Setting the Image Pull Policy for Explicit Roles To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: userArtifacts: image: acme/orders-data/1.0.0 imagePullPolicy: Always - role: proxy images: userArtifacts: image: acme/orders-proxy/1.0.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent Setting the Image Pull Policy for Explicit Roles with Default To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: userArtifacts: imagePullPolicy: Always roles: - role: data images: userArtifacts: image: acme/orders-data/1.0.0 - role: proxy images: userArtifacts: image: acme/orders-proxy/1.0.1 - role: web images: userArtifacts: image: acme/orders-front-end/1.0.1 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Application Image Pull Policy"
        },
        {
            "location": "/clusters/045_storage_enabled",
            "text": " A Coherence cluster member can be storage enabled or storage disabled and hence a CoherenceCluster role can be configured to be storage enabled or disabled. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/045_storage_enabled",
            "text": " Coherence has a default System property that configures cache services to be storage enabled (i.e. that JVM will be manage data for caches) or storage disabled (i.e. that member will be not manage data for caches). A role in a CoherenceCluster can be set as storage enabled or disabled using the storageEnabled field; the value is a boolean true or false. Setting this property sets the Coherence JVM system property coherence.distributed.localstorage to true or false. If the storageEnabled field is not specifically set for a role then the coherence.distributed.localstorage property will not be set in the JVMs for that role and Coherence&#8217;s default behaviour will apply. ",
            "title": "Storage Enabled or Disabled Roles"
        },
        {
            "location": "/clusters/045_storage_enabled",
            "text": " When creating a CoherenceCluster with the single implicit role the storageEnabled field is set in the CoherenceCluster spec field. For example <markup lang=\"yaml\" title=\"Storage Enabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: storageEnabled: true The implicit role will be storage enabled <markup lang=\"yaml\" title=\"Storage Disabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: storageEnabled: false The implicit role will be storage disabled ",
            "title": "Storage Enabled or Disabled Implicit Role"
        },
        {
            "location": "/clusters/045_storage_enabled",
            "text": " When creating a CoherenceCluster with the explicit roles the storageEnabled field is set for each role in the CoherenceCluster roles list. <markup lang=\"yaml\" title=\"Storage Enabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data storageEnabled: true - role: proxy storageEnabled: false The data role will be storage enabled The proxy role will be storage disabled ",
            "title": "Storage Enabled or Disabled Explicit Roles"
        },
        {
            "location": "/clusters/045_storage_enabled",
            "text": " When creating a CoherenceCluster with the explicit roles the storageEnabled field is set for each role in the CoherenceCluster roles list and a default can be set in the CoherenceCluster spec . <markup lang=\"yaml\" title=\"Storage Enabled Role\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: storageEnabled: false roles: - role: data storageEnabled: true - role: proxy - role: web The default value will be storage disabled The data role overrides the default and will be storage enabled The proxy role does not have a specific storageEnabled so will be storage disabled The web roles does not have a specific storageEnabled so will be storage disabled ",
            "title": "Storage Enabled or Disabled Explicit Roles With Defaults"
        },
        {
            "location": "/install/02_prerequisites",
            "text": " Everything needed to install and run the Coherence Operator is listed below: ",
            "title": "preambule"
        },
        {
            "location": "/install/02_prerequisites",
            "text": " In order for the Coherence Operator to be able to install Coherence clusters it needs to be able to pull Coherence Docker images. These images are not available in public Docker repositories and will typically Kubernetes will need authentication to be able to pull them. This is achived by creating pull secrets. Pull secrets are not global and hence secrets will be required in the namespace(s) that Coherence clusters will be installed into. ",
            "title": "Image Pull Secrets"
        },
        {
            "location": "/install/02_prerequisites",
            "text": " Access to a Kubernetes v1.11.3+ cluster. Access to Oracle Coherence Docker images. Image Pull Secrets In order for the Coherence Operator to be able to install Coherence clusters it needs to be able to pull Coherence Docker images. These images are not available in public Docker repositories and will typically Kubernetes will need authentication to be able to pull them. This is achived by creating pull secrets. Pull secrets are not global and hence secrets will be required in the namespace(s) that Coherence clusters will be installed into. ",
            "title": "Prerequisites"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " In the Operator SDK framework a controller is responsible for managing a specific CRD. A single controller could, in theory, manage multiple CRDs but it is clearer and simpler to keep them separate. The Coherence Operator has three controllers, two are part of the operator source code and one is provided by the Operator SDK framework. All controllers have a Reconcile function that is triggered by events from Kubernetes for resources that the controller is listening to. ",
            "title": "Controllers"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The CoherenceCluster controller manages instances of the CoherenceCluster CRD. The source for this controller is in the pkg/controller/coherencecluster/coherencecluster_controller.go file. The CoherenceCluster controller listens for events related to CoherenceCluster CRDs created or modified in the namespace that the operator is running in. It also listens to events for any CoherenceRole CRD that it owns. When a CoherenceCluster resource is created or modified a CoherenceRole is created (or modified or deleted) for each role in the CoherenceCluster spec. Each time a k8s event is raised for a CoherenceCluster or CoherenceRole resource the Reconcile method on the CoherenceCluster controller is called. Create - When a CoherenceCluster is created the controller will work out how many roles are present in the spec. For each role that has a Replica count greater than zero a CoherenceRole is created in k8s. When a CoherenceRole is created it is associated to the parent CoherenceCluster so that k8s can track ownership of related resources (this is used for cascade delete - see below). Update - When a CoherenceCluster is updated the controller will work out what the roles in the updated spec should be. It then compares these roles to the currently deployed CoherenceRoles for that cluster. It then creates, updates or deletes CoherenceRoles as required. Delete - When a CoherenceCluster is deleted the controller does not currently need to do anything. This is because k8s has cascade delete functionality that allows related resources to be deleted together (a little like cascade delete in a database). When a CoherenceCluster is deleted then any related CoherenceRoles will be deleted and also any resources that have those CoherenceRoles as owners (i.e. the corresponding CoherenceInternal resources) ",
            "title": "CoherenceCluster Controller"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The CoherenceRole controller manages instances of the CoherenceRole CRD. The source for this controller is in the pkg/controller/coherencerole/coherencerole_controller.go file. The CoherenceRole controller listens for events related to CoherenceRole CRDs created or modified in the namespace that the operator is running in. It also listens to events for any StatefulSet resources that were created by the corresponding Helm install for the role. When a CoherenceRole resource is created or modified a corresponding CoherenceInternal resource is created (or modified or deleted) from the role&#8217;s spec. Creation of a CoherenceInternal resource will trigger a Helm install of the Coherence Helm chart by the Helm Controller. Each time a k8s event is raised for a CoherenceRole or for a StatefulSet resource related to the role the Reconcile method on the CoherenceRole controller is called. The StatefulSet resource is listened to as a way to keep track of the state fo the role, i.e how many replicas are actually running and ready compared to the desired state. The StatefulSet is also used to obtain references to the Pods that make up the role when performing a StatusHA check prior to scaling. Create - When a CoherenceRole is created a corresponding CoherenceInternal resource will be created in k8s. Update - When a CoherenceRole is updated one of three actions can take place. Scale Up - If the update increases the role&#8217;s replica count then the role is being scaled up. The role&#8217;s spec is first checked to determine whether anything else has changed, if it has a rolling upgrade is performed first to bring the existing members up to the desired spec. After any possible the upgrade then the role&#8217;s member count is scaled up. Scale Down - If the update decreases the role&#8217;s replica count then the role is being scaled down. The member count of the role is scaled down and then the role&#8217;s spec is checked to determine whether anything else has changed, if it has a rolling upgrade is performed to bring the remaining members up to the desired spec. Update Only - If the changes to the role&#8217;s spec do not include a change to the replica count then a rolling upgrade is performed of the existing cluster members. Rolling Upgrade - A rolling upgrade is actually performed out of the box by the StatefulSet associated to the role. To upgrade the members of a role the CoherenceRole controller only has to update the CoherenceInternal spec. This will cause the Helm controller to update the associated Helm install whivh in turn causes the StatefulSet to perform a rolling update of the associated Pods. Scaling - The CoherenceOperator supports safe scaling of the members of a role. This means that a scaling operation will not take place unless the members of the role are Status HA. Safe scaling means that the number of replicas is scaled one at a time untile the desired size is reached with a Status HA check being performed before each member is added or removed. The exact action is controlled by a customer defined scaling policy that is part of the role&#8217;s spec. There are three policy types: SafeScaling - the safe scaling policy means that regardless of whether a role is being scaled up or down the size is always scaled one at a time with a Status HA check before each member is added or removed. ParallelScaling - with parallel scaling no Status HA check is performed, a role is scaled to the desired size by adding or removing the required number of members at the same time. For a storage enabled role with this policy scaling down could result in data loss. Ths policy is intended for storage disabled roles where it allows for fatser start and scaling times. ParallelUpSafeDownScaling - this policy is the default scaling policy. It means that when scaling up the required number of members is added all at once but when scaling down members are removed one at a time with a Status HA check before each removal. This policy allows clusters to start and scale up fatser whilst protecting from data loss when scaling down. Delete - As with a CoherenceCluster, when a CoherenceRole is deleted its corresponding CoherenceInternal resource is also deleted by a cascading delete in k8s. The CoherenceRole controller does not need to take any action on deletion. ",
            "title": "CoherenceRole Controller"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The final controller in the Coherence Operator is the Helm controller. This controller is actually part of the Operator SDK and the source is not in the Coherence Operator&#8217;s source code tree. The Helm controller is configured to watch for a particular CRD and performs Helm install, delete and upgrades as resources based on that CRD are created, deleted or updated. In the case of the Coherence Operator the Helm controller is watching for instances of the CoherenceInternal CRD that are created, updated or deleted by the CoherenceRole controller. When this occurs the Helm controller uses the spec of the CoherenceInternal resource as the values file to install or upgrade the Coherence Helm chart. The Coherence Helm chart used by the operator is actually embedded in the Coherence Operator Docker image so there is no requirement for the customer to have access to a chart repository. The Helm operator also uses an embedded helm and tiller so there is no requirement for the customer to install Helm in their k8s cluster. A customer can have Helm installed but it will never be used by the operator so there is no version conflict. If a customer were to perform a helm ls operation in their cluster they would not see the installs controlled by the Coherence Operator. ",
            "title": "Helm Controller"
        },
        {
            "location": "/developer/04_how_it_works",
            "text": " The high level operation of the Coherence Operator can be seen in the diagram below. The entry point to the operator is the`main()` function in the cmd/manager/main.go file. This function performs the creation and initialisation of the three controllers and the ReST server. It also creates a configuration k8s secret that is used by Coherence Pods. The Coherence Operator works in a single namespace, that is it manages CRDs and hence Coherence clusters only in the same namespace that it is installed into. Controllers In the Operator SDK framework a controller is responsible for managing a specific CRD. A single controller could, in theory, manage multiple CRDs but it is clearer and simpler to keep them separate. The Coherence Operator has three controllers, two are part of the operator source code and one is provided by the Operator SDK framework. All controllers have a Reconcile function that is triggered by events from Kubernetes for resources that the controller is listening to. CoherenceCluster Controller The CoherenceCluster controller manages instances of the CoherenceCluster CRD. The source for this controller is in the pkg/controller/coherencecluster/coherencecluster_controller.go file. The CoherenceCluster controller listens for events related to CoherenceCluster CRDs created or modified in the namespace that the operator is running in. It also listens to events for any CoherenceRole CRD that it owns. When a CoherenceCluster resource is created or modified a CoherenceRole is created (or modified or deleted) for each role in the CoherenceCluster spec. Each time a k8s event is raised for a CoherenceCluster or CoherenceRole resource the Reconcile method on the CoherenceCluster controller is called. Create - When a CoherenceCluster is created the controller will work out how many roles are present in the spec. For each role that has a Replica count greater than zero a CoherenceRole is created in k8s. When a CoherenceRole is created it is associated to the parent CoherenceCluster so that k8s can track ownership of related resources (this is used for cascade delete - see below). Update - When a CoherenceCluster is updated the controller will work out what the roles in the updated spec should be. It then compares these roles to the currently deployed CoherenceRoles for that cluster. It then creates, updates or deletes CoherenceRoles as required. Delete - When a CoherenceCluster is deleted the controller does not currently need to do anything. This is because k8s has cascade delete functionality that allows related resources to be deleted together (a little like cascade delete in a database). When a CoherenceCluster is deleted then any related CoherenceRoles will be deleted and also any resources that have those CoherenceRoles as owners (i.e. the corresponding CoherenceInternal resources) CoherenceRole Controller The CoherenceRole controller manages instances of the CoherenceRole CRD. The source for this controller is in the pkg/controller/coherencerole/coherencerole_controller.go file. The CoherenceRole controller listens for events related to CoherenceRole CRDs created or modified in the namespace that the operator is running in. It also listens to events for any StatefulSet resources that were created by the corresponding Helm install for the role. When a CoherenceRole resource is created or modified a corresponding CoherenceInternal resource is created (or modified or deleted) from the role&#8217;s spec. Creation of a CoherenceInternal resource will trigger a Helm install of the Coherence Helm chart by the Helm Controller. Each time a k8s event is raised for a CoherenceRole or for a StatefulSet resource related to the role the Reconcile method on the CoherenceRole controller is called. The StatefulSet resource is listened to as a way to keep track of the state fo the role, i.e how many replicas are actually running and ready compared to the desired state. The StatefulSet is also used to obtain references to the Pods that make up the role when performing a StatusHA check prior to scaling. Create - When a CoherenceRole is created a corresponding CoherenceInternal resource will be created in k8s. Update - When a CoherenceRole is updated one of three actions can take place. Scale Up - If the update increases the role&#8217;s replica count then the role is being scaled up. The role&#8217;s spec is first checked to determine whether anything else has changed, if it has a rolling upgrade is performed first to bring the existing members up to the desired spec. After any possible the upgrade then the role&#8217;s member count is scaled up. Scale Down - If the update decreases the role&#8217;s replica count then the role is being scaled down. The member count of the role is scaled down and then the role&#8217;s spec is checked to determine whether anything else has changed, if it has a rolling upgrade is performed to bring the remaining members up to the desired spec. Update Only - If the changes to the role&#8217;s spec do not include a change to the replica count then a rolling upgrade is performed of the existing cluster members. Rolling Upgrade - A rolling upgrade is actually performed out of the box by the StatefulSet associated to the role. To upgrade the members of a role the CoherenceRole controller only has to update the CoherenceInternal spec. This will cause the Helm controller to update the associated Helm install whivh in turn causes the StatefulSet to perform a rolling update of the associated Pods. Scaling - The CoherenceOperator supports safe scaling of the members of a role. This means that a scaling operation will not take place unless the members of the role are Status HA. Safe scaling means that the number of replicas is scaled one at a time untile the desired size is reached with a Status HA check being performed before each member is added or removed. The exact action is controlled by a customer defined scaling policy that is part of the role&#8217;s spec. There are three policy types: SafeScaling - the safe scaling policy means that regardless of whether a role is being scaled up or down the size is always scaled one at a time with a Status HA check before each member is added or removed. ParallelScaling - with parallel scaling no Status HA check is performed, a role is scaled to the desired size by adding or removing the required number of members at the same time. For a storage enabled role with this policy scaling down could result in data loss. Ths policy is intended for storage disabled roles where it allows for fatser start and scaling times. ParallelUpSafeDownScaling - this policy is the default scaling policy. It means that when scaling up the required number of members is added all at once but when scaling down members are removed one at a time with a Status HA check before each removal. This policy allows clusters to start and scale up fatser whilst protecting from data loss when scaling down. Delete - As with a CoherenceCluster, when a CoherenceRole is deleted its corresponding CoherenceInternal resource is also deleted by a cascading delete in k8s. The CoherenceRole controller does not need to take any action on deletion. Helm Controller The final controller in the Coherence Operator is the Helm controller. This controller is actually part of the Operator SDK and the source is not in the Coherence Operator&#8217;s source code tree. The Helm controller is configured to watch for a particular CRD and performs Helm install, delete and upgrades as resources based on that CRD are created, deleted or updated. In the case of the Coherence Operator the Helm controller is watching for instances of the CoherenceInternal CRD that are created, updated or deleted by the CoherenceRole controller. When this occurs the Helm controller uses the spec of the CoherenceInternal resource as the values file to install or upgrade the Coherence Helm chart. The Coherence Helm chart used by the operator is actually embedded in the Coherence Operator Docker image so there is no requirement for the customer to have access to a chart repository. The Helm operator also uses an embedded helm and tiller so there is no requirement for the customer to install Helm in their k8s cluster. A customer can have Helm installed but it will never be used by the operator so there is no version conflict. If a customer were to perform a helm ls operation in their cluster they would not see the installs controlled by the Coherence Operator. ",
            "title": "How The Operator Works"
        },
        {
            "location": "/developer/05_building",
            "text": " The Operator SDK generates Go projects that use Go Modules and hence the Coherence Operator uses Go Modules too. The Coherence Operator can be checked out from Git to any location, it does not have to be under your $GOPATH . The first time that the project is built may require Go to fetch a number of dependencies and may take longer than usual to complete. The easiest way to build the whole project is using make . To build the Coherence Operator, package the Helm charts and create the various Docker images run the following command: <markup lang=\"bash\" >make all The all make target will build the Go and Java parts of the Operator and create all of the images required. There have been issues with Go not being able to resolve all of the module dependencies required to build the Coherence Operator. This can be resolved by setting the GOPROXY environment variable GOPROXY=https://proxy.golang.org ",
            "title": "How to Build the Coherence Operator"
        },
        {
            "location": "/developer/05_building",
            "text": " The Coherence Operator contains tests that can be executed using make . The tests are plain Go tests and also Ginkgo test suites. To execute the unit and functional tests that do not require a k8s cluster you can execute the following command: <markup lang=\"bash\" >make test-all This will build and execute all of the Go and Java tests, you do not need to have run a make build first. ",
            "title": "Unit Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " To only tun the Go tests use: <markup lang=\"bash\" >make test-operator ",
            "title": "Go Unit Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " To only tun the Java tests use: <markup lang=\"bash\" >make test-mvn ",
            "title": "Java Unit Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " End to end tests require the Operator to be running. There are three types of end-to-end tests, Helm tests, local tests and remote tests. Helm tests are tests that install the Coherence Operator Helm chart and then make assertions about the state fo the resulting install. These tests do not test functionality of the Operator itself. The Helm tests suite is run using make: make helm-test Local tests, which is the majority ot the tests, can be executed with a locally running operator (i.e. the operator does not need to be deployed in a container in k8s). This makes the tests faster to run and also makes it possible to run the operator in a debugger while the test is executing The local end-to-end test suite is run using make: make e2e-local-test It is possible to run a sub-set of the tests or an individual test by using the GO_TEST_FLAGS=&lt;regex&gt; parameter. For example, to just run the TestMinimalCoherenceCluster clustering test in the test/e2e/local/clustering_test.go file: <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' The reg-ex above matches exactly the TestMinimalCoherenceCluster test name because it uses the reg-ex start ^ and end $ characters. For example, to run all of the clustering tests where the test name starts with TestOneRole we can use the reg-ex ^TestOneRole.*' <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestOneRole.*' Note Any $ signs in the reg-ex need to be escaped by using a double dollar sign $$ . The GO_TEST_FLAGS parameter can actually consist of any valid argument to be passed to the go test command. There is plenty of documentation on go test Remote tests require the operator to actually be installed in a container in k8s. An example of this is the scaling tests because the operator needs to be able to directly reach the Pods. Very few end-to-end tests fall into this categrory. The local end-to-end test suite is run using make: make e2e-test As with local tests the GO_TEST_FLAGS parameter can be used to execute a sub-set of tests or a single test. ",
            "title": "End-to-End Tests"
        },
        {
            "location": "/developer/05_building",
            "text": " Unit Tests The Coherence Operator contains tests that can be executed using make . The tests are plain Go tests and also Ginkgo test suites. To execute the unit and functional tests that do not require a k8s cluster you can execute the following command: <markup lang=\"bash\" >make test-all This will build and execute all of the Go and Java tests, you do not need to have run a make build first. Go Unit Tests To only tun the Go tests use: <markup lang=\"bash\" >make test-operator Java Unit Tests To only tun the Java tests use: <markup lang=\"bash\" >make test-mvn End-to-End Tests End to end tests require the Operator to be running. There are three types of end-to-end tests, Helm tests, local tests and remote tests. Helm tests are tests that install the Coherence Operator Helm chart and then make assertions about the state fo the resulting install. These tests do not test functionality of the Operator itself. The Helm tests suite is run using make: make helm-test Local tests, which is the majority ot the tests, can be executed with a locally running operator (i.e. the operator does not need to be deployed in a container in k8s). This makes the tests faster to run and also makes it possible to run the operator in a debugger while the test is executing The local end-to-end test suite is run using make: make e2e-local-test It is possible to run a sub-set of the tests or an individual test by using the GO_TEST_FLAGS=&lt;regex&gt; parameter. For example, to just run the TestMinimalCoherenceCluster clustering test in the test/e2e/local/clustering_test.go file: <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' The reg-ex above matches exactly the TestMinimalCoherenceCluster test name because it uses the reg-ex start ^ and end $ characters. For example, to run all of the clustering tests where the test name starts with TestOneRole we can use the reg-ex ^TestOneRole.*' <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestOneRole.*' Note Any $ signs in the reg-ex need to be escaped by using a double dollar sign $$ . The GO_TEST_FLAGS parameter can actually consist of any valid argument to be passed to the go test command. There is plenty of documentation on go test Remote tests require the operator to actually be installed in a container in k8s. An example of this is the scaling tests because the operator needs to be able to directly reach the Pods. Very few end-to-end tests fall into this categrory. The local end-to-end test suite is run using make: make e2e-test As with local tests the GO_TEST_FLAGS parameter can be used to execute a sub-set of tests or a single test. ",
            "title": "Testing"
        },
        {
            "location": "/developer/05_building",
            "text": " By default the version number used to tag the Docker images and Helm charts is set in the VERSION property in the Makefile and in the pom.xml files in the java/ directory. The Makefile also contains a VERSION_SUFFIX variable that is used to add a suffix to the build. By default this suffix is ci so the default version of the build artifacts is 2.0.0-ci . Change this suffix, for example when building a release candidate or a full release. For example, if building a release called alpha2 the following command can be used: <markup lang=\"bash\" >make build-all-images VERSION_SUFFIX=alpha2 If building a full release without a suffix the following command can be used <markup lang=\"bash\" >make build-all-images VERSION_SUFFIX=\"\" Testing Unit Tests The Coherence Operator contains tests that can be executed using make . The tests are plain Go tests and also Ginkgo test suites. To execute the unit and functional tests that do not require a k8s cluster you can execute the following command: <markup lang=\"bash\" >make test-all This will build and execute all of the Go and Java tests, you do not need to have run a make build first. Go Unit Tests To only tun the Go tests use: <markup lang=\"bash\" >make test-operator Java Unit Tests To only tun the Java tests use: <markup lang=\"bash\" >make test-mvn End-to-End Tests End to end tests require the Operator to be running. There are three types of end-to-end tests, Helm tests, local tests and remote tests. Helm tests are tests that install the Coherence Operator Helm chart and then make assertions about the state fo the resulting install. These tests do not test functionality of the Operator itself. The Helm tests suite is run using make: make helm-test Local tests, which is the majority ot the tests, can be executed with a locally running operator (i.e. the operator does not need to be deployed in a container in k8s). This makes the tests faster to run and also makes it possible to run the operator in a debugger while the test is executing The local end-to-end test suite is run using make: make e2e-local-test It is possible to run a sub-set of the tests or an individual test by using the GO_TEST_FLAGS=&lt;regex&gt; parameter. For example, to just run the TestMinimalCoherenceCluster clustering test in the test/e2e/local/clustering_test.go file: <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestMinimalCoherenceCluster$$' The reg-ex above matches exactly the TestMinimalCoherenceCluster test name because it uses the reg-ex start ^ and end $ characters. For example, to run all of the clustering tests where the test name starts with TestOneRole we can use the reg-ex ^TestOneRole.*' <markup lang=\"bash\" >make e2e-local-test GO_TEST_FLAGS='-run=^TestOneRole.*' Note Any $ signs in the reg-ex need to be escaped by using a double dollar sign $$ . The GO_TEST_FLAGS parameter can actually consist of any valid argument to be passed to the go test command. There is plenty of documentation on go test Remote tests require the operator to actually be installed in a container in k8s. An example of this is the scaling tests because the operator needs to be able to directly reach the Pods. Very few end-to-end tests fall into this categrory. The local end-to-end test suite is run using make: make e2e-test As with local tests the GO_TEST_FLAGS parameter can be used to execute a sub-set of tests or a single test. ",
            "title": "Build Versions"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " When using the implicit role configuration the Coherence image to use is set directly in the CoherenceCluster spec images.coherence.image` section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4 The coherence container in the implicit role&#8217;s Pod will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4 ",
            "title": "Setting the Coherence Image for the Implicit Role"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " When using the explicit roles in a CoherenceCluster roles list the Coherence image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 The coherence container in the data role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 The coherence container in the proxy role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 ",
            "title": "Setting the Coherence Image for Explicit Roles"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " When using the explicit roles in a CoherenceCluster roles list the Coherence image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data - role: proxy The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the data and the proxy roles. The coherence container in all of the Pods will use this image. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy - role: web The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the proxy and the web roles. The coherence container in all of the Pods will use this image. The container-registry.oracle.com/middleware/coherence/12.2.1.4.1 image is specifically set for the data role so the coherence container in the Pods for the data role will use this image. ",
            "title": "Setting the Coherence Image for Explicit Roles with a Default"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " The Coherence Operator has a default setting for the Coherence image that will be used when by Pods in a CoherenceCluster . This default value can be overridden to enable roles in the cluster to use a different image. If the image being configured is from a registry requiring authentication see the section on pulling from private registries . As well as setting the image name it is also sometimes useful to set the Coherence image&#8217;s image pull policy . Setting the Coherence Image for the Implicit Role When using the implicit role configuration the Coherence image to use is set directly in the CoherenceCluster spec images.coherence.image` section. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4 The coherence container in the implicit role&#8217;s Pod will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4 Setting the Coherence Image for Explicit Roles When using the explicit roles in a CoherenceCluster roles list the Coherence image to use is set for each role. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 The coherence container in the data role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 The coherence container in the proxy role Pods will use the Coherence image container-registry.oracle.com/middleware/coherence/12.2.1.4.1 Setting the Coherence Image for Explicit Roles with a Default When using the explicit roles in a CoherenceCluster roles list the Coherence image to use can be set in the CoherenceCluster spec section and will apply to all roles unless specifically overridden for a role in the roles list. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data - role: proxy The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the data and the proxy roles. The coherence container in all of the Pods will use this image. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy - role: web The image container-registry.oracle.com/middleware/coherence/12.2.1.4.0 set in the spec section will be used by both the proxy and the web roles. The coherence container in all of the Pods will use this image. The container-registry.oracle.com/middleware/coherence/12.2.1.4.1 image is specifically set for the data role so the coherence container in the Pods for the data role will use this image. ",
            "title": "Setting the Coherence Image"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always ",
            "title": "Setting the Image Pull Policy for the Implicit Role"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 imagePullPolicy: Always - role: proxy images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: imagePullPolicy: Always roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: web images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Image Pull Policy for Explicit Roles with Default"
        },
        {
            "location": "/clusters/060_coherence_image",
            "text": " The image pull policy controls when (and if) Kubernetes will pull the Coherence image onto the node where the Coherence Pods are being schedules. See Kubernetes imagePullPolicy for more information. The Kubernetes default pull policy is IfNotPresent unless the image tag is :latest in which case the default policy is Always . The IfNotPresent policy causes the Kubelet to skip pulling an image if it already exists. Note that you should avoid using the :latest tag, see Kubernetes Best Practices for Configuration for more information. The Coherence image&#8217;s pull policy is set using the imagePullPolicy field in the spec.images.coherence section. Setting the Image Pull Policy for the Implicit Role To set the imagePullPolicy for the implicit role: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: Always The image pull policy for the implicit role above has been set to Always Setting the Image Pull Policy for Explicit Roles To set the imagePullPolicy for the explicit roles in the roles list: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 imagePullPolicy: Always - role: proxy images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The image pull policy for the data role has been set to Always The image pull policy for the proxy role above has been set to IfNotPresent Setting the Image Pull Policy for Explicit Roles with Default To set the imagePullPolicy for the explicit roles with a default value: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: images: coherence: imagePullPolicy: Always roles: - role: data images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: proxy images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.1 - role: web images: coherence: image: container-registry.oracle.com/middleware/coherence/12.2.1.4.0 imagePullPolicy: IfNotPresent The default image pull policy is set to Always . The data and proxy roles will use the default value because they do not specifically set the value in their specs. The image pull policy for the web role above has been set to IfNotPresent ",
            "title": "Setting the Coherence Image Pull Policy"
        },
        {
            "location": "/install/04_manual_install",
            "text": " It is possible to install the Coherence Operator by crating the required yaml manually and installing it into Kubernetes. ",
            "title": "preambule"
        },
        {
            "location": "/install/04_manual_install",
            "text": " In Kubernetes clusters with RBAC enabled the Coherence Operator requires certain RBAC resources to be created. <markup lang=\"bash\" >sh examples/create_role.sh The example RBAC script creates a ServiceAccount with the name coherence-operator if a different name is required then the name in the example/example-rbac.yaml should be modified. The shell script above will use default values for the role name, role binding name and install into the default namespace. If the default values need to be changed or the operator is to be installed into a namespace other than default the script can be run with the following environment variables: <markup lang=\"bash\" >export ROLE_NAME=&lt;role-name&gt; export ROLE_BINDING_NAME=&lt;role-binding-name&gt; export NAMESPACE=&lt;namespace&gt; sh example/create_role.sh ",
            "title": "RBAC"
        },
        {
            "location": "/install/04_manual_install",
            "text": " Create a Deployment for the Coherence Operator using the example script. <markup lang=\"bash\" >kubectl -n &lt;namespace&gt; create -f example/example-deployment.yaml where &lt;namespace&gt; is the name of the namespace that the operator is to be deployed into. If RBAC is being used this will also be the same namespace used to create the RBAC roles. If the default namespace is being used the -n &lt;namespace&gt; argument can be omitted. ",
            "title": "Deployment"
        },
        {
            "location": "/guides/020_quickstart",
            "text": " TBD&#8230;&#8203; ",
            "title": "Quick Start Guide"
        },
        {
            "location": "/clusters/030_roles",
            "text": " A CoherenceCluster is made up of one or more roles defined in its spec . ",
            "title": "preambule"
        },
        {
            "location": "/clusters/030_roles",
            "text": " A role is what is actually configured in the CoherenceCluster spec. In a traditional Coherence application that may have had a number of storage enabled members and a number of storage disable Coherence*Extend proxy members this cluster would have effectively had two roles, \"storage\" and \"proxy\". Some clusters may simply have just a storage role and some complex Coherence applications and clusters may have many roles and even different roles storage enabled for different caches/services within the same cluster. The Coherence Operator uses an internal crd named CoherenceRole to represent a role in a Coherence Cluster. A CoherenceRole would not typically be modified directly outside of a handful of specialized operations, such as scaling. Any modification to a role would normally be done by modifying that role in the corresponding CoherenceCluster and leaving the COherence Operator to update the CoherenceRole . ",
            "title": "Define Coherence Roles"
        },
        {
            "location": "/clusters/030_roles",
            "text": " As mentioned previously, all of the fields in a CoherenceCluster spec are optional meaning that the yaml below is perfectly valid. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster This yaml will create an implicit single role with a default role name of storage and a default replica count of three. The implicit role can be modified by specifying role related fields in the CoherenceCluster spec . The role name and replica count of the implicit role can be overridden using the corresponding fields <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 The role name is set with the role field, in this case the role name of the implicit role is now data The replica count is set using the replicas field, in this case the implicit role will now have six replicas. Other role fields can also be used, for example, to set the cache configuration file use by the implicit roles: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 cacheConfig: test-config.xml The cacheConfig field is used to set the cache configuration to test-config.xml . ",
            "title": "Implicit Default Role"
        },
        {
            "location": "/clusters/030_roles",
            "text": " It is possible to also create roles explicitly in the roles list of the CoherenceCluster spec . If creating a Coherence cluster with more than one role then all roles must be defined in the roles list. If creating a Coherence cluster with a single role it is optional whether the specification of that role is put into the CoherenceCluster``spec directly as shown above or whether the single role is added to the roles list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 The yaml above defines a single explicit role in the roles list When defining explict roles the role name is mandatory. The role name is set with the role field, in this case the role name of the role is data The replica count is set using the replicas field, in this case the role will have six replicas. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The yaml above defines a two explicit roles in the roles list The first role has a role name of data and a replica count of six. The second role has a role name of proxy and a replica count of three. ",
            "title": "Explicit Roles"
        },
        {
            "location": "/clusters/030_roles",
            "text": " When defining explicit roles in the roles list any values added to the CoherenceCluster spec directly (where an implicit role would normally be configured) become default values shared by all roles in the roles list unless specifically overridden by a role . This makes it easier to maintain configuration common to all roles in a single location in the spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy The roles list contains two explicit roles, data and proxy The replicas value is set at the spec level and so will be shared by both of the explicit roles In the above example the cluster will have a total of 12 members, six for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 2 Now a new web role has been introduced The replicas count in the spec section will still apply to the data and proxy roles, whch will each have a replica count of 6 The web role has a specific replicas value which will override the spec.replicas value so the web role will have two replicas ",
            "title": "Explicit Roles - Shared Values"
        },
        {
            "location": "/clusters/030_roles",
            "text": " All CoherenceCluster resources will have at lest one role defined. This could be the implicit default role or it could be one more explicit roles. Implicit Default Role As mentioned previously, all of the fields in a CoherenceCluster spec are optional meaning that the yaml below is perfectly valid. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster This yaml will create an implicit single role with a default role name of storage and a default replica count of three. The implicit role can be modified by specifying role related fields in the CoherenceCluster spec . The role name and replica count of the implicit role can be overridden using the corresponding fields <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 The role name is set with the role field, in this case the role name of the implicit role is now data The replica count is set using the replicas field, in this case the implicit role will now have six replicas. Other role fields can also be used, for example, to set the cache configuration file use by the implicit roles: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: role: data replicas: 6 cacheConfig: test-config.xml The cacheConfig field is used to set the cache configuration to test-config.xml . Explicit Roles It is possible to also create roles explicitly in the roles list of the CoherenceCluster spec . If creating a Coherence cluster with more than one role then all roles must be defined in the roles list. If creating a Coherence cluster with a single role it is optional whether the specification of that role is put into the CoherenceCluster``spec directly as shown above or whether the single role is added to the roles list. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 The yaml above defines a single explicit role in the roles list When defining explict roles the role name is mandatory. The role name is set with the role field, in this case the role name of the role is data The replica count is set using the replicas field, in this case the role will have six replicas. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: roles: - role: data replicas: 6 - role: proxy replicas: 3 The yaml above defines a two explicit roles in the roles list The first role has a role name of data and a replica count of six. The second role has a role name of proxy and a replica count of three. Explicit Roles - Shared Values When defining explicit roles in the roles list any values added to the CoherenceCluster spec directly (where an implicit role would normally be configured) become default values shared by all roles in the roles list unless specifically overridden by a role . This makes it easier to maintain configuration common to all roles in a single location in the spec. For example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy The roles list contains two explicit roles, data and proxy The replicas value is set at the spec level and so will be shared by both of the explicit roles In the above example the cluster will have a total of 12 members, six for each role. <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: test-cluster spec: replicas: 6 roles: - role: data - role: proxy - role: web replicas: 2 Now a new web role has been introduced The replicas count in the spec section will still apply to the data and proxy roles, whch will each have a replica count of 6 The web role has a specific replicas value which will override the spec.replicas value so the web role will have two replicas ",
            "title": "Defining a Coherence Role"
        },
        {
            "location": "/clusters/150_volumes",
            "text": " Although a Coherence cluster member may not need access to specific volumes custom applications deployed into a cluster may require them. For this reason it is possible to configure roles in a CoherenceCluster with arbitrary volume mounts. ",
            "title": "preambule"
        },
        {
            "location": "/clusters/150_volumes",
            "text": "",
            "title": "Configure Volumes"
        },
        {
            "location": "/guides/010_overview",
            "text": " QuickStart Follow step-by-step quick start guide to using the Coherence Operator. Install Installing and running the Coherence Operator. ",
            "title": "Getting Started"
        },
        {
            "location": "/guides/010_overview",
            "text": " ReST Management API Managing Coherence clusters with management over ReST. Coherence Metrics Publishing metrics from Coherence clusters. Clusters Managing Coherence clusters. Developer Developer guide for building the Coherence Operator. ",
            "title": "More Guides"
        },
        {
            "location": "/about/02_concepts",
            "text": " The Coherence Operator is a Kubernetes Operator that is used to manage Oracle Coherence clusters in Kubernetes. The Coherence Operator takes on the tasks of that human Dev Ops resource might carry out when managing Coherence clusters, such as configuration, installation, safe scaling, management and metrics. The Coherence Operator is a Go based application built using the Operator SDK . It is distributed as a Docker image and Helm chart for easy installation and configuration. ",
            "title": "What is the Coherence Operator"
        },
        {
            "location": "/about/02_concepts",
            "text": " Traditionally a Coherence cluster is a number of distributed JVMs that communicate to form a single coherent cluster. In Kubernetes this concept still applies but can now be though of as a number of Pods that form a single cluster. Inside each Pod is a JVM running Coherence, or some custom application using Coherence. The Coherence Operator uses a Kubernetes Custom Resource Definition to represent a Coherence cluster (and the roles withing it, see below). Every field in the CoherenceCluster crd Spec is optional so a cluster can be defined by yaml as simple as this: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster The metadata.name field in the CoherenceCluster will be used as the Coherence cluster name and would obviously be unique in a given k8s namespace. The Coherence Operator will use default values for fields that have not been entered, so the above yaml will create a Coherence cluster made up of a StatefulSet with a replica count of 3, so there will be three storage enabled Coherence Pods . ",
            "title": "Coherence Clusters"
        },
        {
            "location": "/about/02_concepts",
            "text": " A Coherence cluster can be made up of a number of Pods that perform different roles. All of the Pods in a given role share the same configuration. At a bare minimum a cluster would have at least one role where Pods are storage enabled. Each role in a Coherence cluster has a name and configuration. A cluster can have zero or many roles defined in the CoherenceCluster crd Spec . It is possible to define common configuration shared by all roles to save duplicating configuration multiple times in the yaml. The Coherence Operator will create a StatefulSet for each role defined in the CoherenceCluster crd yaml. This separation allows roles to be managed and scaled independently from each other. As described above the minimal yaml to define a CoherenceCluster is: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster Although there are no roles described in this yaml the Coherence Operator will create a default role with the name storage and give it a replica count of three. There are two ways to describe the specification of a role in a CoherenceCluster crd depending on whether the cluster created will have a single role or multiple roles. The same configuration to create a single role three member cluster as the minimal yaml could be specified more fully as follows: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: role: storage replicas: 3 The role field specifies the name of the role, in this case stroage . The replicas field defines the number of Pods that will be started for this role, in this case three. If a cluster will have multiple roles they are defined in the spec.roles list; so again the same cluster could be defined more fully as: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: storage replicas: 3 This time the role is defined in the roles section of the yaml. The roles section is a list of one or more role specifications. Multiple roles can be defined by adding more roles with distinct names to the roles list; for example: <markup lang=\"yaml\" >apiVersion: coherence.oracle.com/v1 kind: CoherenceCluster metadata: name: my-cluster spec: roles: - role: storage replicas: 3 - role: web replicas: 2 In this case there are two roles defined, the first names storage , with three replicas and the second named web with two replicas. This will result in a Coherence cluster with a total of five members. The Coherence Operator would create two StatefulSets , one for storage with three Pods and one for web with two Pods . ",
            "title": "Coherence Roles"
        },
        {
            "location": "/developer/08_docs",
            "text": " The Coherence Operator documentation can be built directly from make commands. ",
            "title": "preambule"
        },
        {
            "location": "/developer/08_docs",
            "text": " To build the documentation run <markup lang=\"bash\" >make docs This will build the documentation into the directory build/_output/docs ",
            "title": "Build"
        },
        {
            "location": "/developer/08_docs",
            "text": " To see the results of local changes to the documentation it is possible to run a local web-server that will allow the docs to be viewed in a browser. <markup lang=\"bash\" >make server-docs This will start a local web-server on http://localhost:8080 This is useful to see changes in real time as documentation is edited and re-built. The server does no need to be restarted between documentation builds. The local web-server requires Python ",
            "title": "View"
        },
        {
            "location": "/developer/08_docs",
            "text": " The Coherence Operator documentation is written in Ascii Doc format and is built with tools provided by our friends over in the Helidon team. The documentation source is under the docs/ directory. Build To build the documentation run <markup lang=\"bash\" >make docs This will build the documentation into the directory build/_output/docs View To see the results of local changes to the documentation it is possible to run a local web-server that will allow the docs to be viewed in a browser. <markup lang=\"bash\" >make server-docs This will start a local web-server on http://localhost:8080 This is useful to see changes in real time as documentation is edited and re-built. The server does no need to be restarted between documentation builds. The local web-server requires Python ",
            "title": "Building the Coherence Operator Documentation"
        },
        {
            "location": "/guides/030_applications",
            "text": " Whilst it is simple to deploy a Coherence cluster into Kubernetes in most cases there is also a requirement to add application code and configuration to the Coherence JVMs class path. ",
            "title": "preambule"
        },
        {
            "location": "/guides/030_applications",
            "text": "",
            "title": "Deploying Coherence Applications"
        }
 ]
}