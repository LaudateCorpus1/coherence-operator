///////////////////////////////////////////////////////////////////////////////

    Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

///////////////////////////////////////////////////////////////////////////////

= Configure Pod Scheduling


== Configure Pod Scheduling

In Kubernetes `Pods` can be configured to control how and onto which nodes Kubernetes will schedule those `Pods`; the
Coherence Operator allows the same control for `Pods` in roles in a `CoherenceCluster` resource.

The following settings can be configured:

[cols=2*,options=header]
|===
|Field
|Description

|`nodeSelector`
|`nodeSelector` is the simplest recommended form of node selection constraint.
`nodeSelector` is a field of role spec, it specifies a map of key-value pairs.
For the `Pod` to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels
(it can have additional labels as well).
See https://kubernetes.io/docs/concepts/configuration/assign-pod-node/[Assigning Pods to Nodes] in the
Kubernetes documentation

|`affinity`
|The affinity/anti-affinity feature, greatly expands the types of constraints you can express over just using labels
in a `nodeSelector`.
See https://kubernetes.io/docs/concepts/configuration/assign-pod-node/[Assigning Pods to Nodes] in the
Kubernetes documentation

|`tolerations`
| `nodeSelector` and `affinity` are properties of `Pods` that attracts them to a set of nodes (either as a preference or
a hard requirement). Taints are the opposite â€“ they allow a node to repel a set of `Pods`.
Taints and tolerations work together to ensure that `Pods` are not scheduled onto inappropriate nodes.
One or more taints are applied to a node; this marks that the node should not accept any `Pods` that do not tolerate
the taints. Tolerations are applied to `Pods`, and allow (but do not require) the `Pods` to schedule onto nodes with
matching taints.
See https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/[Taints and Tolerations] in the Kubernetes
documentation.

...
|===

The `nodeSelector`, `affinity` and `tolerations` fields are all part of the role spec and like any other role spec
field can be configured at different levels depending on whether the `CoherenceCluster` has implicit or explicit roles.
The format of the fields is that same as documented in the Kubernetes documentation
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/[Assigning Pods to Nodes] and
https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/[Taints and Tolerations]


== Pod Scheduling for a Single Implicit Role

When configuring a `CoherenceCluster` with a single implicit role the scheduling fields are configured directly in
the `CoherenceCluster` `spec` section.
For example:

[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: CoherenceCluster
metadata:
  name: test-cluster
spec:
  tolerations:                                             # <1>
   - key: "example-key"
     operator: "Exists"
     effect: "NoSchedule"
   nodeSelector:                                           # <2>
   - disktype: ssd
   affinity:                                               # <3>
     nodeAffinity:
       requiredDuringSchedulingIgnoredDuringExecution:
         nodeSelectorTerms:
         - matchExpressions:
           - key: kubernetes.io/e2e-az-name
             operator: In
             values:
             - e2e-az1
             - e2e-az2
----

<1> The `tolerations` are set for the implicit `storage` role
<2> A `nodeSelector` is set for the implicit `storage` role
<3> `affinity` is set for the implicit `storage` role


== Pod Scheduling for Explicit Roles

When configuring one or more explicit roles in a `CoherenceCluster` the scheduling fields are configured for each role
in the `roles` list.
For example

[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: CoherenceCluster
metadata:
  name: test-cluster
spec:
  roles:
    - role: data
      nodeSelector:                                        # <1>
      - disktype: ssd
    - role: proxy
      tolerations:                                         # <2>
       - key: "example-key"
         operator: "Exists"
         effect: "NoSchedule"
       affinity:
         nodeAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
             nodeSelectorTerms:
             - matchExpressions:
               - key: kubernetes.io/e2e-az-name
                 operator: In
                 values:
                 - e2e-az1
                 - e2e-az2
----

<1> The `data` role has a `nodeSelector` configured
<2> The `proxy` role has `tolerations` and `affinity` configured


== Pod Scheduling for Explicit Roles with Defaults

When configuring one or more explicit roles in a `CoherenceCluster` default values for the scheduling fields may be
configured directly in the `spec` section of the `CoherenceCluster` that will apply to all roles in the `roles` list
unless specifically overridden for a role.
Values specified for a role fully override the default values, so even though `nodeSelector` is a map the default and
role values are *not* merged.

For example

[source,yaml]
----
apiVersion: coherence.oracle.com/v1
kind: CoherenceCluster
metadata:
  name: test-cluster
spec:
  nodeSelector:                                        # <1>
    - disktype: ssd
  roles:
    - role: data
      nodeSelector:                                    # <2>
        - shape: massive
    - role: proxy                                      # <3>
    - role: web
----

<1> The default scheduling configuration specified a node selector label of `disktype=ssd`
<2> The `data` role overrides the `nodeSelector` to be `shape=massive`
<3> The `proxy` and `web` roles do not specify any scheduling fields so they will just ue the default node selector
label of `disktype=ssd`

The `tolerations` and `affinity` fields may be used in the same way.

